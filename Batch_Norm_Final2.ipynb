{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5998272e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "f5c77827-0d79-4b61-9d10-0d0e1ab55c3e",
    "outputId": "8b07436c-bd25-4ec7-ebee-c855f1516921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tcn in /opt/conda/lib/python3.7/site-packages (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-addons[tensorflow-gpu] in /opt/conda/lib/python3.7/site-packages (0.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: multivariate_cwru in /opt/conda/lib/python3.7/site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.60.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (1.6.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "%pip install keras-tcn --no-dependencies\n",
    "\n",
    "%pip install tensorflow-addons[tensorflow-gpu] --no-dependencies\n",
    "\n",
    "%pip install pandas --upgrade\n",
    "\n",
    "%pip install multivariate_cwru\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "%pip install scipy\n",
    "\n",
    "%pip install sklearn\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "print(visible_devices)\n",
    "num_GPU = len(tf.config.experimental.list_physical_devices('/physical_device:GPU:0'))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "#Deep Learning pkgs\n",
    "from tensorflow.keras import backend as K, Input, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.activations import swish\n",
    "K.backend()\n",
    "\n",
    "# Python\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "#Project Specific\n",
    "import tcn_ed\n",
    "from tcn_ed import TCN, tcn_full_summary, compiled_tcn\n",
    "from help_pre import create_data_batcht as Create_Batch, create_pred_batch\n",
    "import multivariate_cwru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329dcd64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "860ba9d3-9bb6-4604-bdd5-667b337ae7d4",
    "outputId": "ff970372-2a15-46f5-f0d6-ca61a1afcf3b"
   },
   "outputs": [],
   "source": [
    "#mirrored_strategy = tf.distribute.Mi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305d1cc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "gradient": {},
    "id": "2Dkdg6KoEVe6",
    "outputId": "ac70f14a-75d3-42e0-c663-63e8c34310d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/CWRU_TCN'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b160e0",
   "metadata": {
    "gradient": {},
    "id": "87e01dfb-3302-4081-8311-20afa03e5780",
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_dir = Path('.')\n",
    "DATA1_PATH = Path(\"./Datatest\")\n",
    "DATA_PATH = Path(\"./Datatest/CWRU\")\n",
    "save_model_path = working_dir / 'Model' \n",
    "DE_path = DATA_PATH / '12DriveEndFault'\n",
    "DE_path1 = DE_path / '1730'\n",
    "DE_path2 = DE_path / '1750'\n",
    "DE_path3 = DE_path / '1772'\n",
    "DE_path4 = DE_path / '1797'\n",
    "\n",
    "FE_path = DATA_PATH / '12FanEndFault'\n",
    "FE_path1 = FE_path / '1730'\n",
    "FE_path2 = FE_path / '1750'\n",
    "FE_path3 = FE_path / '1772'\n",
    "FE_path4 = FE_path / '1797'\n",
    "\n",
    "DE48_path = DATA_PATH / '48DriveEndFault'\n",
    "DE48_path1 = DE48_path / '1730'\n",
    "DE48_path2 = DE48_path / '1750'\n",
    "DE48_path3 = DE48_path / '1772'\n",
    "DE48_path4 = DE48_path / '1797'\n",
    "\n",
    "Normal_path = DATA_PATH / 'NormalBaseline'\n",
    "Normal_path1 = Normal_path / '1730'\n",
    "Normal_path2 = Normal_path / '1750'\n",
    "Normal_path3 = Normal_path / '1772'\n",
    "Normal_path4 = Normal_path / '1797'\n",
    "\n",
    "val_path = DATA1_PATH / 'for_pred'\n",
    "val_DE_path = val_path / '12DriveEndFault'\n",
    "val_DE_path1 = val_DE_path / '1730'\n",
    "val_DE_path2 = val_DE_path / '1750'\n",
    "val_DE_path3 = val_DE_path / '1772'\n",
    "val_DE_path4 = val_DE_path / '1797'\n",
    "\n",
    "val_FE_path = val_path / '12FanEndFault'\n",
    "val_FE_path1 = val_FE_path / '1730'\n",
    "val_FE_path2 = val_FE_path / '1750'\n",
    "val_FE_path3 = val_FE_path / '1772'\n",
    "val_FE_path4 = val_FE_path / '1797'\n",
    "\n",
    "val_DE48_path = val_path / '48DriveEndFault'\n",
    "val_DE48_path1 = val_DE48_path / '1730'\n",
    "val_DE48_path2 = val_DE48_path / '1750'\n",
    "val_DE48_path3 = val_DE48_path / '1772'\n",
    "val_DE48_path4 = val_DE48_path / '1797'\n",
    "\n",
    "val_Normal_path = val_path / 'NormalBaseline'\n",
    "val_Normal_path1 = val_Normal_path / '1730'\n",
    "val_Normal_path2 = val_Normal_path / '1750'\n",
    "val_Normal_path3 = val_Normal_path / '1772'\n",
    "val_Normal_path4 = val_Normal_path / '1797'\n",
    "\n",
    "Val_path= [val_DE_path1, val_DE_path2, val_DE_path3, val_DE_path4, val_Normal_path4, val_FE_path1, val_FE_path2, val_FE_path3, val_FE_path4]\n",
    "\n",
    "#Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, DE48_path1, DE48_path2,  DE48_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "data_path = Paths\n",
    "\n",
    "b_size = 1024 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "split_perc = 0.3\n",
    "\n",
    "for path in [DATA_PATH, save_model_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "        \n",
    "data_path = Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a7a4c7",
   "metadata": {
    "gradient": {},
    "id": "325ed5a0-ea45-41f0-b4ab-65880af67bdc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with mirrored_strategy.scope():\n",
    "    \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.3,   \n",
    "                              patience=40, \n",
    "                              min_lr=0.000000015,\n",
    "                              verbose=1, \n",
    "                              cooldown=5)\n",
    "\n",
    "ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                         min_delta=0.00001, \n",
    "                                         patience=70, \n",
    "                                         verbose=1, \n",
    "                                         mode='auto', \n",
    "                                         baseline=None, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "callback = [reduce_lr, ES_cb]\n",
    "\n",
    "def adjust_dilations(dilations: list):\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations\n",
    "        \n",
    "def pred_accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.cast(y_pred, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "def create_model(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "\n",
    "    input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "    def get_opt():\n",
    "        if opt == 'adam':\n",
    "            return optimizers.Adam(lr=lr)\n",
    "        elif opt == 'rmsprop':\n",
    "            return optimizers.RMSprop(lr=lr)\n",
    "        else:\n",
    "            raise Exception('Only Adam and RMSProp are available here')\n",
    "\n",
    "    x = TCN(nb_filters=filt_num,\n",
    "            kernel_size=kernel_num,\n",
    "            nb_stacks=stack,\n",
    "            dilations=[2 ** i for i in range(dilation)],\n",
    "            padding='causal',\n",
    "            use_skip_connections=use_skip,\n",
    "            dropout_rate=drop_rate,\n",
    "            return_sequences=False,\n",
    "            activation='swish', \n",
    "            kernel_initializer='he_uniform',\n",
    "            use_batch_norm=False,\n",
    "            use_layer_norm=False,\n",
    "            use_weight_norm=False,\n",
    "            name='Model')(input_layer)\n",
    "\n",
    "    tcn = TCN(nb_filters=filt_num,\n",
    "              kernel_size=kernel_num,\n",
    "              nb_stacks=stack,\n",
    "              dilations=[2 ** i for i in range(dilation)],\n",
    "              padding='causal',\n",
    "              use_skip_connections=use_skip,\n",
    "              dropout_rate=drop_rate,\n",
    "              return_sequences=False,\n",
    "              activation='swish', \n",
    "              kernel_initializer='he_uniform',\n",
    "              use_batch_norm=False,\n",
    "              use_layer_norm=False,\n",
    "              use_weight_norm=False,\n",
    "              name='Model')\n",
    "\n",
    "    print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "    # classification\n",
    "    x = Dense(num_classes)(x)\n",
    "    x = Activation('softmax', dtype='float32')(x)\n",
    "    output_layer = x\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    # https://github.com/keras-team/keras/pull/11373\n",
    "    # It's now in Keras@master but still not available with pip.\n",
    "    # TODO remove later.\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    print('model.x = {}'.format(input_layer.shape))\n",
    "    print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "    print('Filter Length: %s' % filt_num)\n",
    "    print('Kernel Size: %s' % kernel_num)\n",
    "    print('Dilation: %s' % dilation)\n",
    "    print('Learning Rate: %s' % lr)\n",
    "    print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_func(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.5,   \n",
    "                                  patience=15, \n",
    "                                  min_lr=0.000015,\n",
    "                                  verbose=0, \n",
    "                                  cooldown=5)\n",
    "\n",
    "    ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                             min_delta=0.00001, \n",
    "                                             patience=70, \n",
    "                                             verbose=0, \n",
    "                                             mode='auto', \n",
    "                                             baseline=None, \n",
    "                                             restore_best_weights=True)\n",
    "    \n",
    "\n",
    "    # to load best weights model.load_weights(latest)\n",
    "\n",
    "    callback = ES_cb# ] [reduce_lr, \n",
    "    #seg_length = None\n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    history = model.fit(train_data,\n",
    "              epochs=runs,\n",
    "              verbose=2,\n",
    "              callbacks=callback,\n",
    "              validation_data=test_dataset)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    y_preds = model.predict(Val_dataset,\n",
    "                                   verbose=1,\n",
    "                                   callbacks=callback,\n",
    "                                  )\n",
    "    y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "    predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "    #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "    #y_argmax = []\n",
    "    #i=0\n",
    "    perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "    Accuracy_test = float(perc_score)\n",
    "    print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "    con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "    class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "    print(con_mat, '\\n\\n')\n",
    "    print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "    \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return history, y_preds, append_list2_in\n",
    "\n",
    "def create_model2(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    #x =Sequential()\n",
    "\n",
    "\n",
    "    input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "    def get_opt():\n",
    "        if opt == 'adam':\n",
    "            return optimizers.Adam(lr=lr)\n",
    "        elif opt == 'rmsprop':\n",
    "            return optimizers.RMSprop(lr=lr)\n",
    "        else:\n",
    "            raise Exception('Only Adam and RMSProp are available here')\n",
    "\n",
    "    x = TCN(nb_filters=filt_num,\n",
    "            kernel_size=kernel_num,\n",
    "            nb_stacks=stack,\n",
    "            dilations=[2 ** i for i in range(dilation)],\n",
    "            padding='causal',\n",
    "            use_skip_connections=use_skip,\n",
    "            dropout_rate=drop_rate,\n",
    "            return_sequences=False,\n",
    "            activation='swish', \n",
    "            kernel_initializer='he_uniform',\n",
    "            use_batch_norm=True,\n",
    "            use_layer_norm=False,\n",
    "            use_weight_norm=False,\n",
    "            name='Model')(input_layer)\n",
    "\n",
    "    tcn = TCN(nb_filters=filt_num,\n",
    "              kernel_size=kernel_num,\n",
    "              nb_stacks=stack,\n",
    "              dilations=[2 ** i for i in range(dilation)],\n",
    "              padding='causal',\n",
    "              use_skip_connections=use_skip,\n",
    "              dropout_rate=drop_rate,\n",
    "              return_sequences=False,\n",
    "              activation='swish', \n",
    "              kernel_initializer='he_uniform',\n",
    "              use_batch_norm=True,\n",
    "              use_layer_norm=False,\n",
    "              use_weight_norm=False,\n",
    "              name='Model')\n",
    "\n",
    "    print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "    # classification\n",
    "    x = Dense(num_classes)(x)\n",
    "    x = Activation('softmax', dtype='float32')(x)\n",
    "    output_layer = x\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    # https://github.com/keras-team/keras/pull/11373\n",
    "    # It's now in Keras@master but still not available with pip.\n",
    "    # TODO remove later.\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    print('model.x = {}'.format(input_layer.shape))\n",
    "    print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "    print('Filter Length: %s' % filt_num)\n",
    "    print('Kernel Size: %s' % kernel_num)\n",
    "    print('Dilation: %s' % dilation)\n",
    "    print('Learning Rate: %s' % lr)\n",
    "    print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_func2(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}.H5'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    \n",
    "\n",
    "    #seg_length = None\n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    history = model.fit(train_data,\n",
    "              epochs=runs,\n",
    "              verbose=1,\n",
    "              callbacks=callback,\n",
    "              validation_data=test_dataset)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    y_preds = model.predict(Val_dataset,\n",
    "                                   verbose=0,\n",
    "                                   callbacks=callback,\n",
    "                                  )\n",
    "    y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "    predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "    #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "    #y_argmax = []\n",
    "    #i=0\n",
    "    perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "    Accuracy_test = float(perc_score)\n",
    "    print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "    con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "    class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "    print(con_mat, '\\n\\n')\n",
    "    print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "       \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return model, history, y_preds, append_list2_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85948e",
   "metadata": {
    "id": "3999ae05-e01c-42ab-b94b-e60e34df9e0f"
   },
   "source": [
    "```'N':0, 'B':1, 'IR':2, 'OR':3```\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebc8f08a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "153bc396-4658-4733-a0da-28538f2399e4",
    "outputId": "0718584f-69bc-4f29-fdbb-44d712b98742",
    "tags": []
   },
   "source": [
    "#b_size = 256 #normally 2048 but 512 on colab\n",
    "#b_size2 = int(b_size / 4)\n",
    "#step_size = 300\n",
    "#step_length=step_size\n",
    "#segment_length = 2400\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "waste_split = 0.1\n",
    "\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff730b7",
   "metadata": {
    "id": "3fb548e3-948f-4df7-942f-d90ec7ab68eb"
   },
   "source": [
    "# Batch Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba460b0",
   "metadata": {
    "id": "0fd369cd-6f1f-4962-999f-b2598bebcb3c"
   },
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "444d42fb",
   "metadata": {
    "gradient": {}
   },
   "source": [
    "# Too big for 16GB\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "b_size = 2048 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "#with mirrored_strategy.scope():\n",
    "   # train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    #val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "append_list_time1 = pd.DataFrame()\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    #Tuned Param dropout\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.175\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 1\n",
    "    print('Model Number %s' % m_count)\n",
    "    model, history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                                    test_dataset=test_dataset,\n",
    "                                                    Val_dataset=val_dataset,\n",
    "                                                    predict_labels=predict_labels, \n",
    "                                                    filt_num=16,\n",
    "                                                    kernel_num=11,\n",
    "                                                    dilation=7,\n",
    "                                                    stack=1,\n",
    "                                                    learn_r=Learning_Rate,\n",
    "                                                    drop_rate=Drop_rate,\n",
    "                                                    runs=Repeats,\n",
    "                                                    Model_num = m_count, \n",
    "                                                    segment_length = segment_length)\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.3\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 2\n",
    "    print('Model Number %s' % m_count)\n",
    "    model, history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                                    test_dataset=test_dataset,\n",
    "                                                    Val_dataset=val_dataset,\n",
    "                                                    predict_labels=predict_labels, \n",
    "                                                    filt_num=16,\n",
    "                                                    kernel_num=11,\n",
    "                                                    dilation=7,\n",
    "                                                    stack=1,\n",
    "                                                    learn_r=Learning_Rate,\n",
    "                                                    drop_rate=Drop_rate,\n",
    "                                                    runs=Repeats,\n",
    "                                                    Model_num = m_count, \n",
    "                                                    segment_length = segment_length)\n",
    "\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e21cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_list_time1=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd1556",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number 0\n",
      "Time till start of create model 4.655600059777498e-05\n",
      "Receptive Field Size: 2541\n",
      "x.shape= (None, 16)\n",
      "model.x = (None, 2400, 1)\n",
      "model.y = (None, 4)\n",
      "Filter Length: 16\n",
      "Kernel Size: 11\n",
      "Dilation: 7\n",
      "Learning Rate: 0.00125\n",
      "Dropout Rate: 0.175\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2400, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Model (TCN)                  (None, 16)                37040     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 37,108\n",
      "Trainable params: 37,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Time to create model 0.5622988329996588\n",
      "Epoch 1/75\n",
      "55/55 [==============================] - 59s 959ms/step - loss: 1.5021 - sparse_categorical_accuracy: 0.3866 - val_loss: 1.1827 - val_sparse_categorical_accuracy: 0.4032\n",
      "Epoch 2/75\n",
      "55/55 [==============================] - 49s 897ms/step - loss: 1.1565 - sparse_categorical_accuracy: 0.4532 - val_loss: 0.9403 - val_sparse_categorical_accuracy: 0.4775\n",
      "Epoch 3/75\n",
      "55/55 [==============================] - 49s 897ms/step - loss: 0.8907 - sparse_categorical_accuracy: 0.5842 - val_loss: 0.8269 - val_sparse_categorical_accuracy: 0.5703\n",
      "Epoch 4/75\n",
      "55/55 [==============================] - 49s 896ms/step - loss: 0.7581 - sparse_categorical_accuracy: 0.6328 - val_loss: 0.7861 - val_sparse_categorical_accuracy: 0.5976\n",
      "Epoch 5/75\n",
      "55/55 [==============================] - 49s 896ms/step - loss: 0.6976 - sparse_categorical_accuracy: 0.6516 - val_loss: 0.7551 - val_sparse_categorical_accuracy: 0.6079\n",
      "Epoch 6/75\n",
      "25/55 [============>.................] - ETA: 23s - loss: 0.6569 - sparse_categorical_accuracy: 0.6751"
     ]
    }
   ],
   "source": [
    "b_size = 1024 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "split_perc = 0.3\n",
    "#with mirrored_strategy.scope():\n",
    "train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    \n",
    "waste_split = 0.1\n",
    "\n",
    "\n",
    "#with mirrored_strategy.scope():\n",
    "val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "    \n",
    "append_list_time1=pd.DataFrame()\n",
    "#with mirrored_strategy.scope():\n",
    "append_list_time1=pd.DataFrame()\n",
    "#Tuned Param dropout\n",
    "Learning_Rate = 0.00125\n",
    "Drop_rate = 0.175\n",
    "Repeats = 75\n",
    "\n",
    "m_count = 0\n",
    "print('Model Number %s' % m_count)\n",
    "model0, history0, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    \n",
    "               \n",
    "\n",
    "#with mirrored_strategy.scope():    \n",
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.00125\n",
    "Drop_rate = 0.3\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model1, history1, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d767f5d",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "b_size = 512 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "#with mirrored_strategy.scope():\n",
    "train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "\n",
    "#with mirrored_strategy.scope():\n",
    "#append_list_time1=pd.DataFrame()\n",
    "#Tuned Param dropout\n",
    "Learning_Rate = 0.00125\n",
    "Drop_rate = 0.175\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model2, history2, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    \n",
    "               \n",
    "\n",
    "#with mirrored_strategy.scope():    \n",
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.00125\n",
    "Drop_rate = 0.3\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model3, history3, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.003\n",
    "Drop_rate = 0.3\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model4, history4, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    \n",
    "               \n",
    "\n",
    "#with mirrored_strategy.scope():    \n",
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.006\n",
    "Drop_rate = 0.3\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model5, history5, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4185cc59",
   "metadata": {
    "gradient": {},
    "id": "d94f12ce-e8bb-4d13-9cfd-5b9ea08fddf3"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "b_size = 256 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "#with mirrored_strategy.scope():\n",
    "train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "#with mirrored_strategy.scope():\n",
    "#append_list_time1=pd.DataFrame()\n",
    "#Tuned Param dropout\n",
    "Learning_Rate = 0.003\n",
    "Drop_rate = 0.175\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model6, history6, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    \n",
    "               \n",
    "\n",
    "#with mirrored_strategy.scope():    \n",
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.003\n",
    "Drop_rate = 0.3\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model7, history7, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Learning_Rate = 0.006\n",
    "Drop_rate = 0.175\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model8, history8, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    \n",
    "               \n",
    "\n",
    "#with mirrored_strategy.scope():    \n",
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.01\n",
    "Drop_rate = 0.3\n",
    "Repeats = 75\n",
    "\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model9, history9, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=16,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1a058",
   "metadata": {
    "gradient": {},
    "id": "cd4d5748-a3e9-449d-8b54-030e531aa8b1"
   },
   "outputs": [],
   "source": [
    "append_list_time1.sort_values(by=['segment length', 'kernel size', 'stacks', 'dilation', 'filters'], ascending=[True, False, False, False, True])\n",
    "append_list_time1.to_csv(r'{}.csv'.format('List_Check_300'), index = False, header=['segment length', 'filters', 'kernel size', 'stacks', 'dropout', 'lr', 'dilation','Training Time', 'train loss', 'train acc', 'eval acc','Eval Acc','C1 correct', 'C1 as C2', 'C1 as C3', 'C1 as C4','C2 as C1', 'C2 correct', 'C2 as C3', 'C2 as C4','C3 as C1', 'C3 as C2', 'C3 correct', 'C3 as C4','C4 as C1', 'C4 as C2', 'C4 as C3', 'C4 correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3178a30",
   "metadata": {
    "gradient": {},
    "id": "8a1570ba-1772-4251-827f-fef44b4e1a50"
   },
   "outputs": [],
   "source": [
    "append_list_time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415da89",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
