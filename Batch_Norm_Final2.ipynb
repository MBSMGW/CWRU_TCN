{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59b56a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "f5c77827-0d79-4b61-9d10-0d0e1ab55c3e",
    "outputId": "8b07436c-bd25-4ec7-ebee-c855f1516921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tcn in /opt/conda/lib/python3.7/site-packages (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-addons[tensorflow-gpu] in /opt/conda/lib/python3.7/site-packages (0.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: multivariate_cwru in /opt/conda/lib/python3.7/site-packages (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.60.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (1.6.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scipy) (1.19.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "%pip install keras-tcn --no-dependencies\n",
    "\n",
    "%pip install tensorflow-addons[tensorflow-gpu] --no-dependencies\n",
    "\n",
    "%pip install pandas --upgrade\n",
    "\n",
    "%pip install multivariate_cwru\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "%pip install scipy\n",
    "\n",
    "%pip install sklearn\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "print(visible_devices)\n",
    "num_GPU = len(tf.config.experimental.list_physical_devices('/physical_device:GPU:0'))\n",
    "#if num_GPU == 1:\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[:], True)\n",
    "#elif num_GPU == 2:\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#elif num_GPU == 4:\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#print(\"Num GPUs Available: \", num_GPU)\n",
    "#tf.debugging.set_log_device_placement(False)\n",
    "#mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:1\"])\n",
    "#strategy = tf.distribute.MirroredStrategy(\n",
    "    #cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "# Data science libraries\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "#Deep Learning pkgs\n",
    "from tensorflow.keras import backend as K, Input, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.activations import swish\n",
    "K.backend()\n",
    "\n",
    "# Python\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "#Project Specific\n",
    "import tcn_ed\n",
    "from tcn_ed import TCN, tcn_full_summary, compiled_tcn\n",
    "from help_pre import create_data_batcht as Create_Batch, create_pred_batch\n",
    "import multivariate_cwru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e18e6798",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "860ba9d3-9bb6-4604-bdd5-667b337ae7d4",
    "outputId": "ff970372-2a15-46f5-f0d6-ca61a1afcf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117039ba",
   "metadata": {
    "collapsed": true,
    "id": "3NP8ruU8ENeQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1730/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1750/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1772/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12DriveEndFault/1797/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/NormalBaseline/1730/Normal.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/NormalBaseline/1750/Normal.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/NormalBaseline/1772/Normal.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/NormalBaseline/1797/Normal.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.014-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1730/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.014-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1750/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.014-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1772/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.014-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/12FanEndFault/1797/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1797/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1750/0.021-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.007-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.007-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.007-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.007-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.007-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.014-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.014-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.014-OuterRace6.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.021-Ball.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.021-InnerRace.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.021-OuterRace12.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.021-OuterRace3.mat'\n",
      "Downloading to: '/home/jupyter/Datasets/CWRU/48DriveEndFault/1730/0.021-OuterRace6.mat'\n"
     ]
    }
   ],
   "source": [
    "data1 = multivariate_cwru.CWRU(\"12DriveEndFault\", 1000000, 1, 1,2,'1797',\"1797\",\"1772\",\"1750\",\"1730\",normal_condition = True)\n",
    "data2 = multivariate_cwru.CWRU(\"12FanEndFault\", 2400, 1, 1,2,'1797',\"1797\",\"1772\",\"1750\",\"1730\",normal_condition = True)\n",
    "data3 = multivariate_cwru.CWRU(\"48DriveEndFault\", 1000000, 1, 1,2,\"1797\",normal_condition = True)\n",
    "#data = multivariate_cwru.CWRU(\"48DriveEndFault\", 2400, 0, 1,2,\"1772\",normal_condition = True) # Throws an error\n",
    "data4 = multivariate_cwru.CWRU(\"48DriveEndFault\", 1000000, 1, 1,2,\"1750\",normal_condition = True)\n",
    "data5 = multivariate_cwru.CWRU(\"48DriveEndFault\", 1000000, 1, 1,2,\"1730\",normal_condition = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868d2a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "gradient": {},
    "id": "2Dkdg6KoEVe6",
    "outputId": "ac70f14a-75d3-42e0-c663-63e8c34310d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/CWRU_TCN'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1724128",
   "metadata": {
    "gradient": {},
    "id": "87e01dfb-3302-4081-8311-20afa03e5780",
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_dir = Path('.')\n",
    "DATA1_PATH = Path(\"./Datasets\")\n",
    "DATA_PATH = Path(\"./Datasets/CWRU\")\n",
    "save_model_path = working_dir / 'Model' \n",
    "DE_path = DATA_PATH / '12DriveEndFault'\n",
    "DE_path1 = DE_path / '1730'\n",
    "DE_path2 = DE_path / '1750'\n",
    "DE_path3 = DE_path / '1772'\n",
    "DE_path4 = DE_path / '1797'\n",
    "\n",
    "FE_path = DATA_PATH / '12FanEndFault'\n",
    "FE_path1 = FE_path / '1730'\n",
    "FE_path2 = FE_path / '1750'\n",
    "FE_path3 = FE_path / '1772'\n",
    "FE_path4 = FE_path / '1797'\n",
    "\n",
    "DE48_path = DATA_PATH / '48DriveEndFault'\n",
    "DE48_path1 = DE48_path / '1730'\n",
    "DE48_path2 = DE48_path / '1750'\n",
    "DE48_path3 = DE48_path / '1772'\n",
    "DE48_path4 = DE48_path / '1797'\n",
    "\n",
    "Normal_path = DATA_PATH / 'NormalBaseline'\n",
    "Normal_path1 = Normal_path / '1730'\n",
    "Normal_path2 = Normal_path / '1750'\n",
    "Normal_path3 = Normal_path / '1772'\n",
    "Normal_path4 = Normal_path / '1797'\n",
    "\n",
    "#Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, DE48_path1, DE48_path2,  DE48_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "data_path = Paths\n",
    "\n",
    "val_path = DATA1_PATH / 'for_pred'\n",
    "Val_path= [val_path]\n",
    "\n",
    "\n",
    "\n",
    "segment_length = 2400 # The length of the window used to take sub-arrays from each file\n",
    "step_size = 300 # the amount the window moves along the sub-array\n",
    "split_perc = 0.2 # percentage of data to put towards the test set\n",
    "b_size = 1024\n",
    "b_size1 = int(b_size*2)\n",
    "#b_size2 = int(b_size / 4)\n",
    "\n",
    "#step_length = step_size\n",
    "\n",
    "for path in [DATA_PATH, save_model_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "        \n",
    "data_path = Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd56594",
   "metadata": {
    "gradient": {},
    "id": "325ed5a0-ea45-41f0-b4ab-65880af67bdc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.3,   \n",
    "                                  patience=40, \n",
    "                                  min_lr=0.000000015,\n",
    "                                  verbose=1, \n",
    "                                  cooldown=5)\n",
    "\n",
    "    ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                             min_delta=0.00001, \n",
    "                                             patience=70, \n",
    "                                             verbose=1, \n",
    "                                             mode='auto', \n",
    "                                             baseline=None, \n",
    "                                             restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "    callback = [reduce_lr, ES_cb]\n",
    "\n",
    "def adjust_dilations(dilations: list):\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations\n",
    "        \n",
    "def pred_accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.cast(y_pred, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "def create_model(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "\n",
    "        input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "        def get_opt():\n",
    "            if opt == 'adam':\n",
    "                return optimizers.Adam(lr=lr)\n",
    "            elif opt == 'rmsprop':\n",
    "                return optimizers.RMSprop(lr=lr)\n",
    "            else:\n",
    "                raise Exception('Only Adam and RMSProp are available here')\n",
    "            \n",
    "        x = TCN(nb_filters=filt_num,\n",
    "                kernel_size=kernel_num,\n",
    "                nb_stacks=stack,\n",
    "                dilations=[2 ** i for i in range(dilation)],\n",
    "                padding='causal',\n",
    "                use_skip_connections=use_skip,\n",
    "                dropout_rate=drop_rate,\n",
    "                return_sequences=False,\n",
    "                activation='swish', \n",
    "                kernel_initializer='he_uniform',\n",
    "                use_batch_norm=True,\n",
    "                use_layer_norm=False,\n",
    "                use_weight_norm=False,\n",
    "                name='Model')(input_layer)\n",
    "\n",
    "        tcn = TCN(nb_filters=filt_num,\n",
    "                  kernel_size=kernel_num,\n",
    "                  nb_stacks=stack,\n",
    "                  dilations=[2 ** i for i in range(dilation)],\n",
    "                  padding='causal',\n",
    "                  use_skip_connections=use_skip,\n",
    "                  dropout_rate=drop_rate,\n",
    "                  return_sequences=False,\n",
    "                  activation='swish', \n",
    "                  kernel_initializer='he_uniform',\n",
    "                  use_batch_norm=True,\n",
    "                  use_layer_norm=False,\n",
    "                  use_weight_norm=False,\n",
    "                  name='Model')\n",
    "\n",
    "        print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "        print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax', dtype='float32')(x)\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO remove later.\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "        print('model.x = {}'.format(input_layer.shape))\n",
    "        print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "        print('Filter Length: %s' % filt_num)\n",
    "        print('Kernel Size: %s' % kernel_num)\n",
    "        print('Dilation: %s' % dilation)\n",
    "        print('Learning Rate: %s' % lr)\n",
    "        print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "def train_func(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.3,   \n",
    "                                  patience=40, \n",
    "                                  min_lr=0.000015,\n",
    "                                  verbose=0, \n",
    "                                  cooldown=5)\n",
    "\n",
    "    ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                             min_delta=0.00001, \n",
    "                                             patience=70, \n",
    "                                             verbose=0, \n",
    "                                             mode='auto', \n",
    "                                             baseline=None, \n",
    "                                             restore_best_weights=True)\n",
    "    \n",
    "\n",
    "    # to load best weights model.load_weights(latest)\n",
    "\n",
    "    callback = ES_cb# ] [reduce_lr, \n",
    "    #seg_length = None\n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    with mirrored_strategy.scope():\n",
    "        history = model.fit(train_data,\n",
    "                  epochs=runs,\n",
    "                  verbose=2,\n",
    "                  callbacks=callback,\n",
    "                  validation_data=test_dataset)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    with mirrored_strategy.scope():\n",
    "        y_preds = model.predict(Val_dataset,\n",
    "                                       verbose=1,\n",
    "                                       callbacks=callback,\n",
    "                                      )\n",
    "        y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "        predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "        #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "        #y_argmax = []\n",
    "        #i=0\n",
    "        perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "        Accuracy_test = float(perc_score)\n",
    "        print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "        con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "        class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "        print(con_mat, '\\n\\n')\n",
    "        print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "    \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return history, y_preds, append_list2_in\n",
    "\n",
    "def create_model2(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        #x =Sequential()\n",
    "\n",
    "        \n",
    "        input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "        def get_opt():\n",
    "            if opt == 'adam':\n",
    "                return optimizers.Adam(lr=lr)\n",
    "            elif opt == 'rmsprop':\n",
    "                return optimizers.RMSprop(lr=lr)\n",
    "            else:\n",
    "                raise Exception('Only Adam and RMSProp are available here')\n",
    "            \n",
    "        x = TCN(nb_filters=filt_num,\n",
    "                kernel_size=kernel_num,\n",
    "                nb_stacks=stack,\n",
    "                dilations=[2 ** i for i in range(dilation)],\n",
    "                padding='causal',\n",
    "                use_skip_connections=use_skip,\n",
    "                dropout_rate=drop_rate,\n",
    "                return_sequences=False,\n",
    "                activation='swish', \n",
    "                kernel_initializer='he_uniform',\n",
    "                use_batch_norm=True,\n",
    "                use_layer_norm=False,\n",
    "                use_weight_norm=False,\n",
    "                name='Model')(input_layer)\n",
    "\n",
    "        tcn = TCN(nb_filters=filt_num,\n",
    "                  kernel_size=kernel_num,\n",
    "                  nb_stacks=stack,\n",
    "                  dilations=[2 ** i for i in range(dilation)],\n",
    "                  padding='causal',\n",
    "                  use_skip_connections=use_skip,\n",
    "                  dropout_rate=drop_rate,\n",
    "                  return_sequences=False,\n",
    "                  activation='swish', \n",
    "                  kernel_initializer='he_uniform',\n",
    "                  use_batch_norm=True,\n",
    "                  use_layer_norm=False,\n",
    "                  use_weight_norm=False,\n",
    "                  name='Model')\n",
    "\n",
    "        print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "        print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax', dtype='float32')(x)\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO remove later.\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "        print('model.x = {}'.format(input_layer.shape))\n",
    "        print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "        print('Filter Length: %s' % filt_num)\n",
    "        print('Kernel Size: %s' % kernel_num)\n",
    "        print('Dilation: %s' % dilation)\n",
    "        print('Learning Rate: %s' % lr)\n",
    "        print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "def train_func2(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}.H5'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    \n",
    "\n",
    "    #seg_length = None\n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    with mirrored_strategy.scope():\n",
    "        history = model.fit(train_data,\n",
    "                  epochs=runs,\n",
    "                  verbose=2,\n",
    "                  callbacks=callback,\n",
    "                  validation_data=test_dataset)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    with mirrored_strategy.scope():\n",
    "        y_preds = model.predict(Val_dataset,\n",
    "                                       verbose=0,\n",
    "                                       callbacks=callback,\n",
    "                                      )\n",
    "        y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "        predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "        #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "        #y_argmax = []\n",
    "        #i=0\n",
    "        perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "        Accuracy_test = float(perc_score)\n",
    "        print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "        con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "        class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "        print(con_mat, '\\n\\n')\n",
    "        print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "       \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return model, history, y_preds, append_list2_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49bed17",
   "metadata": {
    "id": "3999ae05-e01c-42ab-b94b-e60e34df9e0f"
   },
   "source": [
    "```'N':0, 'B':1, 'IR':2, 'OR':3```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fdc367",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {},
    "id": "153bc396-4658-4733-a0da-28538f2399e4",
    "outputId": "0718584f-69bc-4f29-fdbb-44d712b98742",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sig Divide Time: 1.186478853225708\n",
      "Sig Divide Time: 4.908093690872192\n",
      "class balance of train frame: 3    31861\n",
      "2    15915\n",
      "1    14333\n",
      "0     8849\n",
      "Name: label, dtype: int64\n",
      "class balance of validation (test) frame: 3    7169\n",
      "1    4775\n",
      "2    3185\n",
      "0    2415\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "b_size = 256 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "waste_split = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa135d3",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sig Divide Time: 1.586183786392212\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cb726",
   "metadata": {
    "id": "3fb548e3-948f-4df7-942f-d90ec7ab68eb"
   },
   "source": [
    "# Batch Norm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0790df06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d62323b-d48e-4217-8a88-9b4e6cd2b9a6",
    "outputId": "50c24f3c-151f-4a84-b36c-ee1f9a77f65e",
    "tags": []
   },
   "source": [
    "tf.keras.backend.clear_session()\n",
    "Learning_Rate = 0.0125\n",
    "Drop_rate = 0.175\n",
    "Repeats = 75\n",
    "m_count =2\n",
    "print('Model Number %s' % m_count)\n",
    "with mirrored_strategy.scope():\n",
    "    b_size = 256 #normally 2048 but 512 on colab\n",
    "    b_size2 = int(b_size / 4)\n",
    "    step_size = 300\n",
    "    step_length=step_size\n",
    "    segment_length = 2400\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "    model2, history2, y_preds2, res_list2 = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list2, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "674e1e36",
   "metadata": {
    "id": "e838e208-64cd-4fc4-b916-1bcb3fc29448",
    "outputId": "66dd8a54-5229-4da3-fd49-921b7eb2a5bd",
    "tags": []
   },
   "source": [
    "m_count =2\n",
    "print('Model Number %s' % m_count)\n",
    "tf.keras.backend.clear_session()\n",
    "with mirrored_strategy.scope():\n",
    "    model2, history2, y_preds2, res_list2 = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=0.3,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list3, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098c1c1",
   "metadata": {
    "gradient": {},
    "id": "cd4d5748-a3e9-449d-8b54-030e531aa8b1"
   },
   "outputs": [],
   "source": [
    "append_list_time1.sort_values(by=['segment length', 'kernel size', 'stacks', 'dilation', 'filters'], ascending=[True, False, False, False, True])\n",
    "append_list_time1.to_csv(r'{}.csv'.format('List_Check_300'), index = False, header=['segment length', 'filters', 'kernel size', 'stacks', 'dropout', 'lr', 'dilation','Training Time', 'train loss', 'train acc', 'eval acc','Eval Acc','C1 correct', 'C1 as C2', 'C1 as C3', 'C1 as C4','C2 as C1', 'C2 correct', 'C2 as C3', 'C2 as C4','C3 as C1', 'C3 as C2', 'C3 correct', 'C3 as C4','C4 as C1', 'C4 as C2', 'C4 as C3', 'C4 correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee179d8",
   "metadata": {
    "gradient": {},
    "id": "8a1570ba-1772-4251-827f-fef44b4e1a50"
   },
   "outputs": [],
   "source": [
    "append_list_time1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f476c6a",
   "metadata": {
    "id": "0fd369cd-6f1f-4962-999f-b2598bebcb3c"
   },
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2205f55a",
   "metadata": {
    "collapsed": true,
    "gradient": {},
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sig Divide Time: 2.194899082183838\n",
      "Sig Divide Time: 12.250142335891724\n",
      "Sig Divide Time: 1.5449819564819336\n",
      "Model Number 1\n",
      "Time till start of create model 6.0821010265499353e-05\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Receptive Field Size: 2541\n",
      "x.shape= (None, 16)\n",
      "model.x = (None, 2400, 1)\n",
      "model.y = (None, 4)\n",
      "Filter Length: 16\n",
      "Kernel Size: 11\n",
      "Dilation: 7\n",
      "Learning Rate: 0.0125\n",
      "Dropout Rate: 0.175\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2400, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Model (TCN)                  (None, 16)                37936     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 38,004\n",
      "Trainable params: 37,556\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Time to create model 1.0233023010077886\n",
      "Epoch 1/75\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[2048,2720,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/Model/residual_block_5/conv1D_1/Pad (defined at notebooks/CWRU_TCN/tcn_ed.py:157) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[div_no_nan/ReadVariableOp_1/_178]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[2048,2720,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/Model/residual_block_5/conv1D_1/Pad (defined at notebooks/CWRU_TCN/tcn_ed.py:157) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_10152]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-15ac5bfa6855>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                             \u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRepeats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                             \u001b[0mModel_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                             segment_length = segment_length)\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mappend_list_time1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_list_time1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mLearning_Rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dc37f7475012>\u001b[0m in \u001b[0;36mtrain_func2\u001b[0;34m(train_data, test_dataset, Val_dataset, predict_labels, segment_length, runs, filt_num, kernel_num, dilation, stack, learn_r, drop_rate, Model_num)\u001b[0m\n\u001b[1;32m    361\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                   validation_data=test_dataset)\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[2048,2720,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/Model/residual_block_5/conv1D_1/Pad (defined at notebooks/CWRU_TCN/tcn_ed.py:157) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[div_no_nan/ReadVariableOp_1/_178]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[2048,2720,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/Model/residual_block_5/conv1D_1/Pad (defined at notebooks/CWRU_TCN/tcn_ed.py:157) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_10152]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# Too big for 16GB\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "b_size = 2048 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "append_list_time1 = pd.DataFrame()\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    #Tuned Param dropout\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.175\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 1\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.3\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 2\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc5b440",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sig Divide Time: 1.670323371887207\n",
      "Sig Divide Time: 7.745492219924927\n",
      "Sig Divide Time: 1.1999897956848145\n",
      "Model Number 1\n",
      "Time till start of create model 6.114200004958548e-05\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Receptive Field Size: 2541\n",
      "x.shape= (None, 16)\n",
      "model.x = (None, 2400, 1)\n",
      "model.y = (None, 4)\n",
      "Filter Length: 16\n",
      "Kernel Size: 11\n",
      "Dilation: 7\n",
      "Learning Rate: 0.00125\n",
      "Dropout Rate: 0.175\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2400, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Model (TCN)                  (None, 16)                37936     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 38,004\n",
      "Trainable params: 37,556\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Time to create model 0.8475955179999346\n",
      "Epoch 1/75\n",
      "108/108 - 133s - loss: 1.2367 - sparse_categorical_accuracy: 0.4556 - val_loss: 1.2414 - val_sparse_categorical_accuracy: 0.3737\n",
      "Epoch 2/75\n",
      "108/108 - 108s - loss: 1.0096 - sparse_categorical_accuracy: 0.5390 - val_loss: 1.1854 - val_sparse_categorical_accuracy: 0.4116\n",
      "Epoch 3/75\n",
      "108/108 - 108s - loss: 0.8875 - sparse_categorical_accuracy: 0.5933 - val_loss: 1.1456 - val_sparse_categorical_accuracy: 0.4729\n",
      "Epoch 4/75\n",
      "108/108 - 108s - loss: 0.7417 - sparse_categorical_accuracy: 0.6669 - val_loss: 1.1961 - val_sparse_categorical_accuracy: 0.3887\n",
      "Epoch 5/75\n",
      "108/108 - 108s - loss: 0.5849 - sparse_categorical_accuracy: 0.7471 - val_loss: 1.2667 - val_sparse_categorical_accuracy: 0.4464\n",
      "Epoch 6/75\n",
      "108/108 - 108s - loss: 0.4728 - sparse_categorical_accuracy: 0.8007 - val_loss: 1.0794 - val_sparse_categorical_accuracy: 0.4906\n",
      "Epoch 7/75\n",
      "108/108 - 108s - loss: 0.3788 - sparse_categorical_accuracy: 0.8443 - val_loss: 1.1128 - val_sparse_categorical_accuracy: 0.5225\n",
      "Epoch 8/75\n",
      "108/108 - 108s - loss: 0.3168 - sparse_categorical_accuracy: 0.8745 - val_loss: 1.0281 - val_sparse_categorical_accuracy: 0.5761\n",
      "Epoch 9/75\n",
      "108/108 - 108s - loss: 0.2674 - sparse_categorical_accuracy: 0.8966 - val_loss: 0.6430 - val_sparse_categorical_accuracy: 0.7717\n",
      "Epoch 10/75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-98246c4f12ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                             \u001b[0mruns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRepeats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                                             \u001b[0mModel_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                             segment_length = segment_length)\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mappend_list_time1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_list_time1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mLearning_Rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-dc37f7475012>\u001b[0m in \u001b[0;36mtrain_func2\u001b[0;34m(train_data, test_dataset, Val_dataset, predict_labels, segment_length, runs, filt_num, kernel_num, dilation, stack, learn_r, drop_rate, Model_num)\u001b[0m\n\u001b[1;32m    361\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                   \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                   validation_data=test_dataset)\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "b_size = 1024 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    #Tuned Param dropout\n",
    "    Learning_Rate = 0.00125\n",
    "    Drop_rate = 0.175\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 1\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.3\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 2\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4f124",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "b_size = 512 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    #Tuned Param dropout\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.175\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 1\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.3\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 2\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064b740",
   "metadata": {
    "gradient": {},
    "id": "d94f12ce-e8bb-4d13-9cfd-5b9ea08fddf3"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "b_size = 256 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, y_s1 = Create_Batch(data_path, split_perc, segment_length, step_length, b_size)\n",
    "    val_dataset, X_v1, predict_labels= create_pred_batch(Val_path=Val_path, segment_length=segment_length, step_length=step_size, b_size=int(b_size/4))\n",
    "waste_split = 0.1\n",
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    #Tuned Param dropout\n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.175\n",
    "    Repeats = 75\n",
    "\n",
    "    m_count = 1\n",
    "    append_list_time1 = pd.DataFrame\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "    \n",
    "    Learning_Rate = 0.0125\n",
    "    Drop_rate = 0.3\n",
    "    Repeats = 75\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    m_count = 2\n",
    "    append_list_time1 = pd.DataFrame\n",
    "    print('Model Number %s' % m_count)\n",
    "    history, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "    append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e7404",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
