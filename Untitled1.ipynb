{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "#%pip install keras-tcn --no-dependencies\n",
    "\n",
    "#%pip install tensorflow-addons[tensorflow-gpu] --no-dependencies\n",
    "\n",
    "#%pip install pandas --upgrade\n",
    "\n",
    "#%pip install multivariate_cwru\n",
    "\n",
    "#%pip install tqdm\n",
    "\n",
    "#%pip install scipy\n",
    "\n",
    "#%pip install sklearn\n",
    "\n",
    "#%pip install matplotlib\n",
    "\n",
    "#%pip install typeguard\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "print(visible_devices)\n",
    "num_GPU = len(tf.config.experimental.list_physical_devices('/physical_device:GPU:0'))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "#Deep Learning pkgs\n",
    "from tensorflow.keras import backend as K, Input, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.activations import swish\n",
    "K.backend()\n",
    "\n",
    "# Python\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "#Project Specific\n",
    "import tcn_ed\n",
    "from tcn_ed import TCN, tcn_full_summary, compiled_tcn\n",
    "from help_pre import create_data_batcht as Create_Batch, create_pred_batch\n",
    "import multivariate_cwru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "install_needed = False  # should only be True once\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U sagemaker\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import ProfilerConfig\n",
    "\n",
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedStratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# Others\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import mixed_precision\n",
    "tf.random.set_seed(1234)\n",
    "seed = 1235\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def matfile_to_dft(folder_path, drop=None):\n",
    "    '''\n",
    "    Read all the matlab files in the folder, preprocess, and return a DataFrame\n",
    "    \n",
    "    Parameter:\n",
    "        folder_path: \n",
    "            Path (Path object) of the folder which contains the matlab files.\n",
    "        drop:\n",
    "            List of column names to be dropped. \n",
    "            Default =  ['BA_time', 'FE_time', 'RPM', 'ans', 'i']\n",
    "    Return:\n",
    "        DataFrame with preprocessed data\n",
    "    '''\n",
    "    if drop == None:\n",
    "        drop = ['BA_time', 'RPM', 'ans', 'i']\n",
    "    dic = matfile_to_dict(folder_path)\n",
    "    remove_dic_items(dic)\n",
    "    rename_keys(dic)\n",
    "    df = pd.DataFrame.from_dict(dic).T\n",
    "    df = df.reset_index().rename(mapper={'index':'filename'},axis=1)\n",
    "    df['label'] = df['filename'].apply(label)\n",
    "    return df.drop(drop, axis=1, errors='ignore')\n",
    "\n",
    "def matfile_to_dict(folder_path):\n",
    "\n",
    "    output_dic = {}\n",
    "    for _, filepath in enumerate(folder_path.glob('*.mat')):\n",
    "        # strip the folder path and get the filename only.\n",
    "        key_name = str(filepath).split('\\\\')[-1]\n",
    "        output_dic[key_name] = scipy.io.loadmat(filepath, squeeze_me=True)\n",
    "    return output_dic\n",
    "\n",
    "\n",
    "def remove_dic_items(dic):\n",
    "    '''\n",
    "    Remove redundant data in the dictionary returned by matfile_to_dic inplace.\n",
    "    '''\n",
    "    # For each file in the dictionary, delete the redundant key-value pairs\n",
    "    for _, values in dic.items():\n",
    "        del values['__header__']\n",
    "        del values['__version__']    \n",
    "        del values['__globals__']\n",
    "        \n",
    "        \n",
    "        \n",
    "def rename_keys(dic):\n",
    "    '''\n",
    "    Rename some keys so that they can be loaded into a \n",
    "    DataFrame with consistent column names\n",
    "    '''\n",
    "    # For each file in the dictionary\n",
    "    for _,v1 in dic.items():\n",
    "        # For each key-value pair, rename the following keys \n",
    "        for k2,_ in list(v1.items()):\n",
    "            if 'DE_time' in k2:\n",
    "                v1['DE_time'] = v1.pop(k2)\n",
    "            elif 'BA_time' in k2:\n",
    "                v1['BA_time'] = v1.pop(k2)\n",
    "            elif 'FE_time' in k2:\n",
    "                v1['FE_time'] = v1.pop(k2)\n",
    "            elif 'RPM' in k2:\n",
    "                v1['RPM'] = v1.pop(k2)\n",
    "                \n",
    "def label(filename):\n",
    "    '''\n",
    "    Function to create label for each signal based on the filename. Apply this\n",
    "    to the \"filename\" column of the DataFrame.\n",
    "    Usage:\n",
    "        df['label'] = df['filename'].apply(label)\n",
    "    '''\n",
    "    #if 'B' in filename:\n",
    "        #print('B %s' % filename)\n",
    "        #return 'B'\n",
    "    #elif 'IR' in filename:\n",
    "        #print('IR %s' % filename)\n",
    "        #return 'IR'\n",
    "    #elif 'OR' in filename:\n",
    "        #print('OR %s' % filename)\n",
    "        #return 'OR'\n",
    "    #elif 'Normal' in filename:\n",
    "        #print('N %s' % filename)\n",
    "        #return 'N'\n",
    "    if 'Inner' in filename:\n",
    "        #print('IR %s' % filename)\n",
    "        return 'IR'\n",
    "    elif 'Outer' in filename:\n",
    "        #print('OR %s' % filename)\n",
    "        return 'OR'\n",
    "    elif 'Normal' in filename:\n",
    "        #print('N %s' % filename)\n",
    "        return 'N'\n",
    "    elif 'Ball' in filename:\n",
    "        #print('B %s' % filename)\n",
    "        return 'B'\n",
    "    \n",
    "def matfile_to_dft(folder_path, drop=None):\n",
    "    '''\n",
    "    Read all the matlab files in the folder, preprocess, and return a DataFrame\n",
    "    \n",
    "    Parameter:\n",
    "        folder_path: \n",
    "            Path (Path object) of the folder which contains the matlab files.\n",
    "        drop:\n",
    "            List of column names to be dropped. \n",
    "            Default =  ['BA_time', 'FE_time', 'RPM', 'ans', 'i']\n",
    "    Return:\n",
    "        DataFrame with preprocessed data\n",
    "    '''\n",
    "    if drop == None:\n",
    "        drop = ['BA_time', 'RPM', 'ans', 'i']\n",
    "    dic = matfile_to_dict(folder_path)\n",
    "    remove_dic_items(dic)\n",
    "    rename_keys(dic)\n",
    "    df = pd.DataFrame.from_dict(dic).T\n",
    "    df = df.reset_index().rename(mapper={'index':'filename'},axis=1)\n",
    "    df['label'] = df['filename'].apply(label)\n",
    "    return df.drop(drop, axis=1, errors='ignore')\n",
    "\n",
    "def matfile_to_dict(folder_path):\n",
    "\n",
    "    output_dic = {}\n",
    "    for _, filepath in enumerate(folder_path.glob('*.mat')):\n",
    "        # strip the folder path and get the filename only.\n",
    "        key_name = str(filepath).split('\\\\')[-1]\n",
    "        output_dic[key_name] = scipy.io.loadmat(filepath, squeeze_me=True)\n",
    "    return output_dic\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #return X_test_frame, y_test_frame\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sig_divide(segment_length, step_length, frame):\n",
    "    create_frame_start = time.time()\n",
    "    n_sample_points = []\n",
    "    splits = []\n",
    "    split_range = []\n",
    "    num_seq = []\n",
    "    for i in range(frame.shape[0]):\n",
    "        n_sample_points.append(len(frame.iloc[i,1]))\n",
    "    n_sample_points = np.transpose(n_sample_points)\n",
    "\n",
    "    nmin1_sample_points = n_sample_points - segment_length\n",
    "    nmin1_remainder = nmin1_sample_points % step_length\n",
    "    nmin1_sample_points = nmin1_sample_points - nmin1_remainder\n",
    "    nminrem_sample_points = n_sample_points - nmin1_remainder\n",
    "    n_splits = nmin1_sample_points / step_length\n",
    "    \n",
    "    for i in n_splits:\n",
    "        val = int(i+1)\n",
    "        s_range = range(0, val, 1)\n",
    "        split_range.append(s_range)\n",
    "\n",
    "\n",
    "    num_seq += range(0, len(n_splits), 1)\n",
    "    \n",
    "    dic = {}\n",
    "    idx = 0\n",
    "    for i in num_seq:\n",
    "        split_range_list = split_range[i]\n",
    "        win_start1 = np.multiply(step_length, split_range_list)\n",
    "        win_end1 = np.add(segment_length, np.multiply(step_length, split_range_list))\n",
    "        for segment in list(split_range_list):\n",
    "            dic[idx] = {\n",
    "                'signal': frame.iloc[i,1][win_start1[segment]:win_end1[segment]], # [0 + (step_length * (segment - 1)):segment_length + (step_length * segment)], \n",
    "                'label': frame.iloc[i,2],\n",
    "            }\n",
    "            \n",
    "            idx += 1\n",
    "    df_tmp1 = pd.DataFrame.from_dict(dic,orient='index')\n",
    "    df_output1 = pd.concat([df_tmp1[['label']], pd.DataFrame(np.vstack(df_tmp1[\"signal\"].values)).astype('float32')], axis=1 )\n",
    "    map_label = {'N':0, 'B':1, 'IR':2, 'OR':3}\n",
    "    df_output1['label'] = df_output1['label'].map(map_label).astype('int16')\n",
    "    #print('Sig Divide Time: %s' % (time.time() - create_frame_start))\n",
    "    #print('val dataset: %s' % df_output1.dtypes)\n",
    "    return df_output1\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_framest(data_path, split_perc, segment_length, step_length):\n",
    "    create_frame_start = time.time()\n",
    "    if len(data_path) == 1:\n",
    "        df = matfile_to_dft(data_path[0])\n",
    "    elif len(data_path) == 2:\n",
    "        df = matfile_to_df(data_path[0])\n",
    "        df = df.append(matfile_to_dft(data_path[1]), ignore_index=True)\n",
    "    elif len(data_path) == 3:\n",
    "        df = matfile_to_df(data_path[0])\n",
    "        df = df.append(matfile_to_dft(data_path[1]), ignore_index=True)\n",
    "        df = df.append(matfile_to_dft(data_path[2]), ignore_index=True)\n",
    "    else:\n",
    "        df = matfile_to_dft(data_path[0])\n",
    "        for path in data_path[1:]:\n",
    "            df = df.append(matfile_to_dft(path), ignore_index=True)\n",
    "            \n",
    "    df_filename = df.pop('filename')\n",
    "    df.insert(0, 'file', df_filename.map(lambda x: str(x).split('/')[-1]))\n",
    "    FE_df = pd.DataFrame([df.file, df.FE_time, df.label]).T.rename(columns={'FE_time':'signal'})\n",
    "    FE_df = FE_df.dropna()\n",
    "    DE_df = pd.DataFrame([df.file, df.DE_time, df.label]).T.rename(columns={'DE_time':'signal'})\n",
    "    DE_df = DE_df.dropna()\n",
    "    df = DE_df.append(FE_df, ignore_index=True)\n",
    "    df = df.sample(frac=1, random_state=1)\n",
    "    \n",
    "    features = df.columns[1:]\n",
    "    target = 'label'\n",
    "\n",
    "    test_frame = df.sample(frac = split_perc, random_state=1) # shuffle the dataframe, random_state is a number seed for reproducability\n",
    "    testframe_id_list = test_frame.index\n",
    "    train_frame = df.drop(testframe_id_list)\n",
    "\n",
    "    test_df_output = sig_divide(segment_length, step_length,frame=test_frame)\n",
    "    train_df_output = sig_divide(segment_length, step_length,frame=train_frame)\n",
    "\n",
    "    train_df_output = train_df_output.sample(frac=1)\n",
    "\n",
    "\n",
    "    test_frame = test_df_output.sample(frac=1)\n",
    "    #y_test_frame = y_test_frame.sort_values(by='label', ascending=False)\n",
    "    \n",
    "    \n",
    "    return train_df_output, test_frame, train_df_output['label'].value_counts(), test_frame['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_batch(Val_path, segment_length=8192, step_length=300, b_size=256):\n",
    "    Vdf = matfile_to_dft(Val_path[0])\n",
    "    for valpath in Val_path[1:]: #Index for Posix returning indexed tuple of path\n",
    "        Vdf = Vdf.append(matfile_to_dft(valpath), ignore_index=True)\n",
    "            \n",
    "    vdf_filename = Vdf.pop('filename')\n",
    "    Vdf.insert(0, 'file', vdf_filename.map(lambda v: str(v).split('/')[-1]))\n",
    "    FE_vdf = pd.DataFrame([Vdf.file, Vdf.FE_time, Vdf.label]).T.rename(columns={'FE_time':'signal'})\n",
    "    FE_vdf = FE_vdf.dropna()\n",
    "    DE_vdf = pd.DataFrame([Vdf.file, Vdf.DE_time, Vdf.label]).T.rename(columns={'DE_time':'signal'})\n",
    "    DE_vdf = DE_vdf.dropna()\n",
    "    Vdf = DE_vdf.append(FE_vdf, ignore_index=True)\n",
    "    Vdf = Vdf.sample(frac=1, random_state=2)\n",
    "    \n",
    "    val_features = Vdf.columns[1:]\n",
    "    target_pred = 'label'\n",
    "\t  #test_frame becomes val frame\n",
    "    val_sig = Vdf.sample(frac = 1, random_state=2) # shuffle the dataframe, random_state is a number seed for reproducability\n",
    "    #testframe_id_list = test_frame.index\n",
    "    \n",
    "    val_df_output = sig_divide(segment_length, step_length, val_sig)\n",
    "\n",
    "    val_frame = val_df_output.sample(frac=1)\n",
    "    y_val_frame = val_frame[[target_pred]].copy()\n",
    "    \n",
    "    \n",
    "    return val_frame, y_val_frame.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = Path('.')\n",
    "DATA1_PATH = Path(\"./Datatest\")\n",
    "DATA_PATH = Path(\"./Datatest/CWRU\")\n",
    "save_model_path = working_dir / 'Model' \n",
    "DE_path = DATA_PATH / '12DriveEndFault'\n",
    "DE_path1 = DE_path / '1730'\n",
    "DE_path2 = DE_path / '1750'\n",
    "DE_path3 = DE_path / '1772'\n",
    "DE_path4 = DE_path / '1797'\n",
    "\n",
    "FE_path = DATA_PATH / '12FanEndFault'\n",
    "FE_path1 = FE_path / '1730'\n",
    "FE_path2 = FE_path / '1750'\n",
    "FE_path3 = FE_path / '1772'\n",
    "FE_path4 = FE_path / '1797'\n",
    "\n",
    "DE48_path = DATA_PATH / '48DriveEndFault'\n",
    "DE48_path1 = DE48_path / '1730'\n",
    "DE48_path2 = DE48_path / '1750'\n",
    "DE48_path3 = DE48_path / '1772'\n",
    "DE48_path4 = DE48_path / '1797'\n",
    "\n",
    "Normal_path = DATA_PATH / 'NormalBaseline'\n",
    "Normal_path1 = Normal_path / '1730'\n",
    "Normal_path2 = Normal_path / '1750'\n",
    "Normal_path3 = Normal_path / '1772'\n",
    "Normal_path4 = Normal_path / '1797'\n",
    "\n",
    "val_path = DATA1_PATH / 'for_pred'\n",
    "val_DE_path = val_path / '12DriveEndFault'\n",
    "val_DE_path1 = val_DE_path / '1730'\n",
    "val_DE_path2 = val_DE_path / '1750'\n",
    "val_DE_path3 = val_DE_path / '1772'\n",
    "val_DE_path4 = val_DE_path / '1797'\n",
    "\n",
    "val_FE_path = val_path / '12FanEndFault'\n",
    "val_FE_path1 = val_FE_path / '1730'\n",
    "val_FE_path2 = val_FE_path / '1750'\n",
    "val_FE_path3 = val_FE_path / '1772'\n",
    "val_FE_path4 = val_FE_path / '1797'\n",
    "\n",
    "val_DE48_path = val_path / '48DriveEndFault'\n",
    "val_DE48_path1 = val_DE48_path / '1730'\n",
    "val_DE48_path2 = val_DE48_path / '1750'\n",
    "val_DE48_path3 = val_DE48_path / '1772'\n",
    "val_DE48_path4 = val_DE48_path / '1797'\n",
    "\n",
    "val_Normal_path = val_path / 'NormalBaseline'\n",
    "val_Normal_path1 = val_Normal_path / '1730'\n",
    "val_Normal_path2 = val_Normal_path / '1750'\n",
    "val_Normal_path3 = val_Normal_path / '1772'\n",
    "val_Normal_path4 = val_Normal_path / '1797'\n",
    "\n",
    "val_path= [val_DE_path1, val_DE_path2, val_DE_path3, val_DE_path4, val_Normal_path4, val_FE_path1, val_FE_path2, val_FE_path3, val_FE_path4]\n",
    "val_path_de = [val_DE_path1, val_DE_path2, val_DE_path3, val_DE_path4, val_Normal_path4]\n",
    "\n",
    "#Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, DE48_path1, DE48_path2,  DE48_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]#, DE48_path1, DE48_path2,  DE48_path4]\n",
    "Pathsde = [DE_path1, DE_path2, DE_path3, DE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "Paths48 = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4, DE48_path1, DE48_path2,  DE48_path4]\n",
    "data_path = Paths\n",
    "\n",
    "b_size = 1024 #normally 2048 but 512 on colab\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 8192\n",
    "split_perc = 0.3\n",
    "\n",
    "for path in [DATA_PATH, save_model_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "        \n",
    "data_path = Paths48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_output_forcsv, test_frame_forcsv, train_l, test_l = create_framest(data_path, split_perc, segment_length, step_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame_for_csv, val_l = create_pred_batch(val_path, segment_length=8192, step_length=300, b_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 299450400 into shape (124771,8192,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-356f68dc51d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 299450400 into shape (124771,8192,1)"
     ]
    }
   ],
   "source": [
    "col_ran = [*range(8192)]\n",
    "segment_length = 8192\n",
    "target = 'label'\n",
    "cwd = os.getcwd()\n",
    "col_ran = [*range(8192)]\n",
    "d_t = dict()\n",
    "d_t['label'] = 'int16'\n",
    "d_t['Unnamed: 0'] = 'int16'\n",
    "for col in col_ran:\n",
    "    d_t['%s' % col] =  'float32'\n",
    "\n",
    "train_frame = pd.read_csv((r'{}.csv').format('train_df_w48_8192'), dtype=(d_t))\n",
    "test_frame = pd.read_csv((r'{}.csv').format('test_df_w48_8192'), dtype=(d_t))\n",
    "\n",
    "train_frame = train_frame.set_index(train_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "train_target = train_frame.pop('label')\n",
    "train_x = train_frame.to_numpy().reshape(len(train_frame), segment_length, 1)\n",
    "train_y = train_target.to_numpy().reshape(-1, 1)\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(b_size).with_options(options).cache()\n",
    "test_frame = test_frame.set_index(test_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "test_target = test_frame.pop('label')\n",
    "test_x = test_frame.to_numpy().reshape(len(test_frame), segment_length, 1)\n",
    "test_y = test_target.to_numpy().reshape(-1, 1)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(b_size).with_options(options).cache()\n",
    "print('class balance of train frame: %s' % train_target.value_counts())\n",
    "print('class balance of validation (test) frame: %s' % test_target.value_counts())\n",
    "out_1 = train_dataset\n",
    "out_2 = test_dataset\n",
    "\n",
    "val_frame = pd.read_csv((r'{}.csv').format('pred_df_no48'), dtype=d_t)\n",
    "\n",
    "\n",
    "val_frame = val_frame.set_index(val_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val_target = val_frame.pop('label')\n",
    "print('class balance of train frame: %s' % val_target.value_counts())\n",
    "val_x = val_frame.to_numpy().reshape(len(val_frame), segment_length, 1)\n",
    "val_y = val_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(b_size).with_options(options).cache()\n",
    "\n",
    "out_1 = dataset\n",
    "out_2 = val_y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tr0 = train_labs[0] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Normal',tr0)\n",
    "tr1 = train_labs[1] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Ball',tr1)\n",
    "tr2 = train_labs[2] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Inner',tr2)\n",
    "tr3 = train_labs[3] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Outer',tr3, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frame = pd.read_csv((r'{}.csv').format('pred_df_w48'), dtype=d_t)\n",
    "\n",
    "\n",
    "val_frame = val_frame.set_index(val_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val_target = val_frame.pop('label')\n",
    "print('class balance of train frame: %s' % val_target.value_counts())\n",
    "val_x = val_frame.to_numpy().reshape(len(val_frame), segment_length, 1)\n",
    "val_y = val_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(b_size).with_options(options).cache()\n",
    "\n",
    "out_1 = dataset\n",
    "out_2 = val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{'N':0, 'B':1, 'IR':2, 'OR':3}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    55704\n",
       "1    28071\n",
       "2    27934\n",
       "0     9520\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    29494\n",
       "1    11771\n",
       "2    10662\n",
       "0     9512\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "3        4770\n",
       "1        2384\n",
       "2        2384\n",
       "0        1612\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val Normal 0.14766109336840127\n",
      "Ball 0.21303776066128124\n",
      "Inner 0.21303776066128124\n",
      "Outer 0.42626338530903624 \n",
      "\n",
      "Train Normal 0.07852906482772273\n",
      "Ball 0.23155350617426523\n",
      "Inner 0.2304234135396646\n",
      "Outer 0.45949401545834745 \n",
      "\n",
      "Test Normal 0.15482022819381827\n",
      "Ball 0.22668361353438482\n",
      "Inner 0.20532670864867988\n",
      "Outer 0.5679896778169353 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_labs = val_l\n",
    "train_labs = train_l\n",
    "test_labs = test_l\n",
    "v0 = val_labs[0] / (val_labs[0] + val_labs[1] + val_labs[2] + val_labs[3])\n",
    "print(' Val Normal',v0)\n",
    "v1 = val_labs[1] / (val_labs[0] + val_labs[1] + val_labs[2] + val_labs[3])\n",
    "print('Ball',v1)\n",
    "v2 = val_labs[2] / (val_labs[0] + val_labs[1] + val_labs[2] + val_labs[3])\n",
    "print('Inner',v2)\n",
    "v3 = val_labs[3] / (val_labs[0] + val_labs[1] + val_labs[2] + val_labs[3])\n",
    "print('Outer',v3, '\\n')\n",
    "tr0 = train_labs[0] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Train Normal',tr0)\n",
    "tr1 = train_labs[1] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Ball',tr1)\n",
    "tr2 = train_labs[2] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Inner',tr2)\n",
    "tr3 = train_labs[3] / (train_labs[0] + train_labs[1] + train_labs[2] + train_labs[3])\n",
    "print('Outer',tr3, '\\n')\n",
    "te0 = test_labs[0] / (test_labs[0] + test_labs[1] + test_labs[2] + test_labs[3])\n",
    "print('Test Normal',te0)\n",
    "te1 = test_labs[1] / (test_labs[1] + test_labs[2] + test_labs[3])\n",
    "print('Ball',te1)\n",
    "te2 = test_labs[2] / (test_labs[1] + test_labs[2] + test_labs[3])\n",
    "print('Inner',te2)\n",
    "te3 = test_labs[3] / (test_labs[1] + test_labs[2] + test_labs[3])\n",
    "print('Outer',te3, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_output_forcsv.to_csv(r'{}.csv'.format('train_df_no48'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame_forcsv.to_csv(r'{}.csv'.format('test_df_no48'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame_for_csv.to_csv(r'{}.csv'.format('pred_df_no48'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_output_forcsv, test_frame_forcsv = create_framest(Paths48, split_perc, segment_length, step_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_output_forcsv.to_csv(r'{}.csv'.format('train_df_w48_8192'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame_forcsv.to_csv(r'{}.csv'.format('test_df_w48_8192'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame_for_csv.to_csv(r'{}.csv'.format('pred_df_no48_8192'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
