{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Num GPUs Available:  2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "\n",
    "# Data science libraries\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import cupy as cp\n",
    "import pandas as pd\n",
    "from tensorflow.keras import backend as K, Input, Model, optimizers\n",
    "\n",
    "# Others\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import tcn_ed\n",
    "from tcn_ed import TCN, tcn_full_summary, compiled_tcn\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.activations import swish\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from sklearn import metrics\n",
    "from help_pre import create_data_batcht as Create_Batch, create_pred_batch\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)\n",
    "\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "print(visible_devices)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "K.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = Path('.')\n",
    "DATA1_PATH = Path(\"./Datatest\")\n",
    "DATA_PATH = Path(\"./Datatest/CWRU\")\n",
    "save_model_path = working_dir / 'Model' \n",
    "DE_path = DATA_PATH / '12DriveEndFault'\n",
    "DE_path1 = DE_path / '1730'\n",
    "DE_path2 = DE_path / '1750'\n",
    "DE_path3 = DE_path / '1772'\n",
    "DE_path4 = DE_path / '1797'\n",
    "\n",
    "FE_path = DATA_PATH / '12FanEndFault'\n",
    "FE_path1 = FE_path / '1730'\n",
    "FE_path2 = FE_path / '1750'\n",
    "FE_path3 = FE_path / '1772'\n",
    "FE_path4 = FE_path / '1797'\n",
    "\n",
    "DE48_path = DATA_PATH / '48DriveEndFault'\n",
    "DE48_path1 = DE48_path / '1730'\n",
    "DE48_path2 = DE48_path / '1750'\n",
    "DE48_path3 = DE48_path / '1772'\n",
    "DE48_path4 = DE48_path / '1797'\n",
    "\n",
    "Normal_path = DATA_PATH / 'NormalBaseline'\n",
    "Normal_path1 = Normal_path / '1730'\n",
    "Normal_path2 = Normal_path / '1750'\n",
    "Normal_path3 = Normal_path / '1772'\n",
    "Normal_path4 = Normal_path / '1797'\n",
    "\n",
    "val_path = DATA1_PATH / 'for_pred'\n",
    "val_DE_path = val_path / '12DriveEndFault'\n",
    "val_DE_path1 = val_DE_path / '1730'\n",
    "val_DE_path2 = val_DE_path / '1750'\n",
    "val_DE_path3 = val_DE_path / '1772'\n",
    "val_DE_path4 = val_DE_path / '1797'\n",
    "\n",
    "val_FE_path = val_path / '12FanEndFault'\n",
    "val_FE_path1 = val_FE_path / '1730'\n",
    "val_FE_path2 = val_FE_path / '1750'\n",
    "val_FE_path3 = val_FE_path / '1772'\n",
    "val_FE_path4 = val_FE_path / '1797'\n",
    "\n",
    "val_DE48_path = val_path / '48DriveEndFault'\n",
    "val_DE48_path1 = val_DE48_path / '1730'\n",
    "val_DE48_path2 = val_DE48_path / '1750'\n",
    "val_DE48_path3 = val_DE48_path / '1772'\n",
    "val_DE48_path4 = val_DE48_path / '1797'\n",
    "\n",
    "val_Normal_path = val_path / 'NormalBaseline'\n",
    "val_Normal_path1 = val_Normal_path / '1730'\n",
    "val_Normal_path2 = val_Normal_path / '1750'\n",
    "val_Normal_path3 = val_Normal_path / '1772'\n",
    "val_Normal_path4 = val_Normal_path / '1797'\n",
    "\n",
    "\n",
    "val_path = [val_DE_path1, val_DE_path2, val_DE_path3, val_DE_path4, val_Normal_path4]\n",
    "\n",
    "#Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, DE48_path1, DE48_path2,  DE48_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "Paths = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]#, DE48_path1, DE48_path2,  DE48_path4]\n",
    "Pathsde = [DE_path1, DE_path2, DE_path3, DE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4]\n",
    "Paths48 = [DE_path1, DE_path2, DE_path3, DE_path4, FE_path1, FE_path2, FE_path3, FE_path4, Normal_path1, Normal_path2, Normal_path3, Normal_path4, DE48_path1, DE48_path2,  DE48_path4]\n",
    "data_path = Paths\n",
    "\n",
    "#segment_length = 2400 # The length of the window used to take sub-arrays from each file\n",
    "#step_size = 300 # the amount the window moves along the sub-array\n",
    "split_perc = 0.4 # percentage of data to put towards the test set\n",
    "#b_size = 1024\n",
    "#b_size1 = int(b_size*2)\n",
    "#b_size2 = int(b_size / 4)\n",
    "\n",
    "#step_length = step_size\n",
    "\n",
    "for path in [DATA_PATH, save_model_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "        \n",
    "data_path = Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimedStopping(Callback):\n",
    "    '''Stop training when enough time has passed.\n",
    "    # Arguments\n",
    "        seconds: maximum time before stopping.\n",
    "        verbose: verbosity mode.\n",
    "    '''\n",
    "    def __init__(self, seconds=None, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.seconds = seconds\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if time.time() - self.start_time > self.seconds:\n",
    "            self.model.stop_training = True\n",
    "            if self.verbose:\n",
    "                print('Stopping after %s seconds.' % self.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeStopping(Callback):\n",
    "    \"\"\"Stop training when a specified amount of time has passed.\n",
    "    Args:\n",
    "        seconds: maximum amount of time before stopping.\n",
    "            Defaults to 86400 (1 day).\n",
    "        verbose: verbosity mode. Defaults to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seconds=9000, verbose=0):\n",
    "        super(TimeStopping, self).__init__()\n",
    "\n",
    "        self.seconds = seconds\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.stopping_time = time.time() + self.seconds\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if time.time() >= self.stopping_time:\n",
    "            self.model.stop_training = True\n",
    "            self.stopped_epoch = epoch\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.verbose > 0 and time.time() >= self.stopping_time:\n",
    "            formatted_time = datetime.timedelta(seconds=self.seconds)\n",
    "            msg = 'Timed stopping at epoch {} after training for {}'.format(\n",
    "                self.stopped_epoch + 1, formatted_time)\n",
    "            print(msg)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'seconds': self.seconds,\n",
    "            'verbose': self.verbose,\n",
    "        }\n",
    "\n",
    "        base_config = super(TimeStopping, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def adjust_dilations(dilations: list):\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations\n",
    "\n",
    "#def accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            #if K.ndim(y_true) == K.ndim(y_pred):\n",
    "                #y_true = K.squeeze(y_true, -1)\n",
    "            # convert dense predictions to labels\n",
    "            #y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "            #y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
    "            #return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "        \n",
    "def pred_accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.cast(y_pred, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "def create_model(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        #x =Sequential()\n",
    "\n",
    "        \n",
    "        input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "        def get_opt():\n",
    "            if opt == 'adam':\n",
    "                return optimizers.Adam(lr=lr)\n",
    "            elif opt == 'rmsprop':\n",
    "                return optimizers.RMSprop(lr=lr)\n",
    "            else:\n",
    "                raise Exception('Only Adam and RMSProp are available here')\n",
    "            \n",
    "        x = TCN(nb_filters=filt_num,\n",
    "                kernel_size=kernel_num,\n",
    "                nb_stacks=stack,\n",
    "                dilations=[2 ** i for i in range(dilation)],\n",
    "                padding='causal',\n",
    "                use_skip_connections=use_skip,\n",
    "                dropout_rate=drop_rate,\n",
    "                return_sequences=False,\n",
    "                activation='swish', \n",
    "                kernel_initializer='he_uniform',\n",
    "                use_batch_norm=True,\n",
    "                use_layer_norm=False,\n",
    "                use_weight_norm=False,\n",
    "                name='Model')(input_layer)\n",
    "\n",
    "        tcn = TCN(nb_filters=filt_num,\n",
    "                  kernel_size=kernel_num,\n",
    "                  nb_stacks=stack,\n",
    "                  dilations=[2 ** i for i in range(dilation)],\n",
    "                  padding='causal',\n",
    "                  use_skip_connections=use_skip,\n",
    "                  dropout_rate=drop_rate,\n",
    "                  return_sequences=False,\n",
    "                  activation='swish', \n",
    "                  kernel_initializer='he_uniform',\n",
    "                  use_batch_norm=True,\n",
    "                  use_layer_norm=False,\n",
    "                  use_weight_norm=False,\n",
    "                  name='Model')\n",
    "\n",
    "        print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "        print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax', dtype='float32')(x)\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO remove later.\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy']) #, callbacks=my_callbacks)\n",
    "\n",
    "        print('model.x = {}'.format(input_layer.shape))\n",
    "        print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "        print('Filter Length: %s' % filt_num)\n",
    "        print('Kernel Size: %s' % kernel_num)\n",
    "        print('Dilation: %s' % dilation)\n",
    "        print('Learning Rate: %s' % lr)\n",
    "        print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "def train_func(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        monitor='sparse_categorical_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                  factor=0.3,   \n",
    "                                  patience=4, \n",
    "                                  min_lr=0.00000015,\n",
    "                                  verbose=0, \n",
    "                                  cooldown=5)\n",
    "\n",
    "    ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                             min_delta=0.0000001, \n",
    "                                             patience=60, \n",
    "                                             verbose=0, \n",
    "                                             mode='auto', \n",
    "                                             baseline=None, \n",
    "                                             restore_best_weights=True)\n",
    "    \n",
    "    timer_cb = TimeStopping(seconds=9000, verbose=0)\n",
    "\n",
    "    # to load best weights model.load_weights(latest)\n",
    "\n",
    "    callback = [reduce_lr, ES_cb, cp_callback, timer_cb]\n",
    "    #seg_length = None\n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    with mirrored_strategy.scope():\n",
    "        history = model.fit(train_data,\n",
    "                  epochs=runs,\n",
    "                  verbose=2,\n",
    "                  callbacks=callback,\n",
    "                  validation_data=test_dataset)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    with mirrored_strategy.scope():\n",
    "        y_preds = model.predict(Val_dataset,\n",
    "                                       verbose=1,\n",
    "                                       callbacks=callback,\n",
    "                                      )\n",
    "        y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "        predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "        #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "        #y_argmax = []\n",
    "        #i=0\n",
    "        perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "        Accuracy_test = float(perc_score)\n",
    "        print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "        con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "        class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "        print(con_mat, '\\n\\n')\n",
    "        print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "    \n",
    "    #append_list2_in.to_csv(r'{}.csv'.format(model_path), index = False, header=['filters', 'kernel size', 'dropout', 'lr', 'dilation','Training Time', 'train loss', 'train acc', 'eval acc','Eval Acc','C1 correct', 'C1 as C2', 'C1 as C3', 'C1 as C4','C2 as C1', 'C2 correct', 'C2 as C3', 'C2 as C4','C3 as C1', 'C3 as C2', 'C3 correct', 'C3 as C4','C4 as C1', 'C4 as C2', 'C4 as C3', 'C4 correct'])\n",
    "    #Csv_list = []\n",
    "    #Csv_list.append('Loss: %s , Acc: %s , Prediction Accuracy: %s , Confusion Matrix: %s , Classification Report: %s' % (history.history['loss'][-1], history.history['sparse_categorical_accuracy'][-1], Accuracy_test, con_mat, class_report))\n",
    "    \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return history, y_preds, append_list2_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.0125\n",
    "Drop_rate = 0.3\n",
    "Repeats = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class balance of train frame: 3    19246\n",
      "2    11386\n",
      "1    10613\n",
      "0     4804\n",
      "Name: label, dtype: int64\n",
      "class balance of test frame: 3    15334\n",
      "1     6675\n",
      "0     6416\n",
      "2     5100\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "b_size = 1024\n",
    "b_size2 = int(b_size / 4)\n",
    "step_size = 300\n",
    "segment_length = 4000\n",
    "with mirrored_strategy.scope():\n",
    "    train_dataset, test_dataset, train_count, test_count = Create_Batch(data_path, split_perc, segment_length=segment_length, step_length=step_size, b_size=b_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class balance of val frame: 3    2366\n",
      "0    1600\n",
      "1    1572\n",
      "2     786\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    Val_dataset, predict_labels, val_count = create_pred_batch(val_path, segment_length, step_length=step_size, b_size=b_size2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n"
     ]
    }
   ],
   "source": [
    "print(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.0125\n",
    "Drop_rate = 0.3\n",
    "Repeats = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number 0\n",
      "Time till start of create model 0.00024323999969055876\n",
      "Receptive Field Size: 2541\n",
      "x.shape= (None, 16)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "model.x = (None, 4096, 1)\n",
      "model.y = (None, 4)\n",
      "Filter Length: 16\n",
      "Kernel Size: 11\n",
      "Dilation: 7\n",
      "Learning Rate: 0.0125\n",
      "Dropout Rate: 0.3\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4096, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Model (TCN)                  (None, 16)                74286     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 74,354\n",
      "Trainable params: 37,332\n",
      "Non-trainable params: 37,022\n",
      "_________________________________________________________________\n",
      "Time to create model 1.7539655130021856\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 46 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 46 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "57/57 - 45s - loss: 1.0331 - sparse_categorical_accuracy: 0.5049 - val_loss: 0.7729 - val_sparse_categorical_accuracy: 0.6153\n",
      "Epoch 2/100\n",
      "57/57 - 22s - loss: 0.6257 - sparse_categorical_accuracy: 0.6869 - val_loss: 0.5266 - val_sparse_categorical_accuracy: 0.7505\n",
      "Epoch 3/100\n",
      "57/57 - 22s - loss: 0.4340 - sparse_categorical_accuracy: 0.7895 - val_loss: 0.3714 - val_sparse_categorical_accuracy: 0.7977\n",
      "Epoch 4/100\n",
      "57/57 - 22s - loss: 0.3507 - sparse_categorical_accuracy: 0.8299 - val_loss: 0.3526 - val_sparse_categorical_accuracy: 0.8270\n",
      "Epoch 5/100\n",
      "57/57 - 22s - loss: 0.3120 - sparse_categorical_accuracy: 0.8492 - val_loss: 0.3709 - val_sparse_categorical_accuracy: 0.8051\n",
      "Epoch 6/100\n",
      "57/57 - 22s - loss: 0.2697 - sparse_categorical_accuracy: 0.8739 - val_loss: 0.3099 - val_sparse_categorical_accuracy: 0.8484\n",
      "Epoch 7/100\n",
      "57/57 - 22s - loss: 0.2491 - sparse_categorical_accuracy: 0.8848 - val_loss: 0.2972 - val_sparse_categorical_accuracy: 0.8491\n",
      "Epoch 8/100\n",
      "57/57 - 22s - loss: 0.2294 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.3143 - val_sparse_categorical_accuracy: 0.8586\n",
      "Epoch 9/100\n",
      "57/57 - 22s - loss: 0.2035 - sparse_categorical_accuracy: 0.9128 - val_loss: 0.3552 - val_sparse_categorical_accuracy: 0.8729\n",
      "Epoch 10/100\n",
      "57/57 - 22s - loss: 0.1904 - sparse_categorical_accuracy: 0.9232 - val_loss: 0.3165 - val_sparse_categorical_accuracy: 0.8698\n",
      "Epoch 11/100\n",
      "57/57 - 22s - loss: 0.1705 - sparse_categorical_accuracy: 0.9329 - val_loss: 0.3142 - val_sparse_categorical_accuracy: 0.8869\n",
      "Epoch 12/100\n",
      "57/57 - 22s - loss: 0.1397 - sparse_categorical_accuracy: 0.9482 - val_loss: 0.3537 - val_sparse_categorical_accuracy: 0.8774\n",
      "Epoch 13/100\n",
      "57/57 - 22s - loss: 0.1196 - sparse_categorical_accuracy: 0.9565 - val_loss: 0.3338 - val_sparse_categorical_accuracy: 0.8781\n",
      "Epoch 14/100\n",
      "57/57 - 22s - loss: 0.1106 - sparse_categorical_accuracy: 0.9602 - val_loss: 0.3279 - val_sparse_categorical_accuracy: 0.8956\n",
      "Epoch 15/100\n",
      "57/57 - 22s - loss: 0.1090 - sparse_categorical_accuracy: 0.9614 - val_loss: 0.2978 - val_sparse_categorical_accuracy: 0.8989\n",
      "Epoch 16/100\n",
      "57/57 - 22s - loss: 0.1037 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.2855 - val_sparse_categorical_accuracy: 0.9075\n",
      "Epoch 17/100\n",
      "57/57 - 22s - loss: 0.0974 - sparse_categorical_accuracy: 0.9656 - val_loss: 0.2561 - val_sparse_categorical_accuracy: 0.9138\n",
      "Epoch 18/100\n",
      "57/57 - 22s - loss: 0.0941 - sparse_categorical_accuracy: 0.9667 - val_loss: 0.3035 - val_sparse_categorical_accuracy: 0.9040\n",
      "Epoch 19/100\n",
      "57/57 - 22s - loss: 0.0891 - sparse_categorical_accuracy: 0.9686 - val_loss: 0.2999 - val_sparse_categorical_accuracy: 0.9043\n",
      "Epoch 20/100\n",
      "57/57 - 22s - loss: 0.0864 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.3547 - val_sparse_categorical_accuracy: 0.8906\n",
      "Epoch 21/100\n",
      "57/57 - 22s - loss: 0.0837 - sparse_categorical_accuracy: 0.9718 - val_loss: 0.2937 - val_sparse_categorical_accuracy: 0.9100\n",
      "Epoch 22/100\n",
      "57/57 - 22s - loss: 0.0762 - sparse_categorical_accuracy: 0.9731 - val_loss: 0.2975 - val_sparse_categorical_accuracy: 0.9081\n",
      "Epoch 23/100\n",
      "57/57 - 22s - loss: 0.0718 - sparse_categorical_accuracy: 0.9763 - val_loss: 0.2910 - val_sparse_categorical_accuracy: 0.9106\n",
      "Epoch 24/100\n",
      "57/57 - 22s - loss: 0.0696 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.3260 - val_sparse_categorical_accuracy: 0.8967\n",
      "Epoch 25/100\n",
      "57/57 - 22s - loss: 0.0712 - sparse_categorical_accuracy: 0.9764 - val_loss: 0.2667 - val_sparse_categorical_accuracy: 0.9140\n",
      "Epoch 26/100\n",
      "57/57 - 22s - loss: 0.0698 - sparse_categorical_accuracy: 0.9768 - val_loss: 0.2669 - val_sparse_categorical_accuracy: 0.9148\n",
      "Epoch 27/100\n",
      "57/57 - 22s - loss: 0.0690 - sparse_categorical_accuracy: 0.9774 - val_loss: 0.2728 - val_sparse_categorical_accuracy: 0.9173\n",
      "Epoch 28/100\n",
      "57/57 - 23s - loss: 0.0665 - sparse_categorical_accuracy: 0.9783 - val_loss: 0.2929 - val_sparse_categorical_accuracy: 0.9097\n",
      "Epoch 29/100\n",
      "57/57 - 22s - loss: 0.0636 - sparse_categorical_accuracy: 0.9786 - val_loss: 0.2893 - val_sparse_categorical_accuracy: 0.9107\n",
      "Epoch 30/100\n",
      "57/57 - 22s - loss: 0.0641 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.2627 - val_sparse_categorical_accuracy: 0.9180\n",
      "Epoch 31/100\n",
      "57/57 - 22s - loss: 0.0618 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.2749 - val_sparse_categorical_accuracy: 0.9161\n",
      "Epoch 32/100\n",
      "57/57 - 22s - loss: 0.0613 - sparse_categorical_accuracy: 0.9798 - val_loss: 0.2644 - val_sparse_categorical_accuracy: 0.9178\n",
      "Epoch 33/100\n",
      "57/57 - 22s - loss: 0.0616 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.2609 - val_sparse_categorical_accuracy: 0.9197\n",
      "Epoch 34/100\n",
      "57/57 - 23s - loss: 0.0626 - sparse_categorical_accuracy: 0.9795 - val_loss: 0.2556 - val_sparse_categorical_accuracy: 0.9210\n",
      "Epoch 35/100\n",
      "57/57 - 22s - loss: 0.0596 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.2878 - val_sparse_categorical_accuracy: 0.9141\n",
      "Epoch 36/100\n",
      "57/57 - 22s - loss: 0.0597 - sparse_categorical_accuracy: 0.9802 - val_loss: 0.2747 - val_sparse_categorical_accuracy: 0.9173\n",
      "Epoch 37/100\n",
      "57/57 - 22s - loss: 0.0605 - sparse_categorical_accuracy: 0.9801 - val_loss: 0.2635 - val_sparse_categorical_accuracy: 0.9179\n",
      "Epoch 38/100\n",
      "57/57 - 22s - loss: 0.0583 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.2717 - val_sparse_categorical_accuracy: 0.9174\n",
      "Epoch 39/100\n",
      "57/57 - 23s - loss: 0.0576 - sparse_categorical_accuracy: 0.9810 - val_loss: 0.2734 - val_sparse_categorical_accuracy: 0.9180\n",
      "Epoch 40/100\n",
      "57/57 - 22s - loss: 0.0577 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2665 - val_sparse_categorical_accuracy: 0.9182\n",
      "Epoch 41/100\n",
      "57/57 - 22s - loss: 0.0585 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2752 - val_sparse_categorical_accuracy: 0.9166\n",
      "Epoch 42/100\n",
      "57/57 - 22s - loss: 0.0565 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2756 - val_sparse_categorical_accuracy: 0.9171\n",
      "Epoch 43/100\n",
      "57/57 - 22s - loss: 0.0583 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.2725 - val_sparse_categorical_accuracy: 0.9177\n",
      "Epoch 44/100\n",
      "57/57 - 22s - loss: 0.0583 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.2756 - val_sparse_categorical_accuracy: 0.9174\n",
      "Epoch 45/100\n",
      "57/57 - 22s - loss: 0.0574 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.2747 - val_sparse_categorical_accuracy: 0.9183\n",
      "Epoch 46/100\n",
      "57/57 - 22s - loss: 0.0591 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2655 - val_sparse_categorical_accuracy: 0.9191\n",
      "Epoch 47/100\n",
      "57/57 - 22s - loss: 0.0565 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 48/100\n",
      "57/57 - 22s - loss: 0.0545 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2765 - val_sparse_categorical_accuracy: 0.9172\n",
      "Epoch 49/100\n",
      "57/57 - 22s - loss: 0.0584 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2705 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 50/100\n",
      "57/57 - 22s - loss: 0.0565 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.2692 - val_sparse_categorical_accuracy: 0.9191\n",
      "Epoch 51/100\n",
      "57/57 - 22s - loss: 0.0571 - sparse_categorical_accuracy: 0.9806 - val_loss: 0.2676 - val_sparse_categorical_accuracy: 0.9194\n",
      "Epoch 52/100\n",
      "57/57 - 28s - loss: 0.0618 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.2661 - val_sparse_categorical_accuracy: 0.9195\n",
      "Epoch 53/100\n",
      "57/57 - 39s - loss: 0.0572 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2698 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 54/100\n",
      "57/57 - 45s - loss: 0.0582 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.2641 - val_sparse_categorical_accuracy: 0.9194\n",
      "Epoch 55/100\n",
      "57/57 - 46s - loss: 0.0591 - sparse_categorical_accuracy: 0.9810 - val_loss: 0.2651 - val_sparse_categorical_accuracy: 0.9192\n",
      "Epoch 56/100\n",
      "57/57 - 40s - loss: 0.0558 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2664 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 57/100\n",
      "57/57 - 22s - loss: 0.0567 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 58/100\n",
      "57/57 - 23s - loss: 0.0582 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.2662 - val_sparse_categorical_accuracy: 0.9191\n",
      "Epoch 59/100\n",
      "57/57 - 22s - loss: 0.0585 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.2678 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 60/100\n",
      "57/57 - 22s - loss: 0.0558 - sparse_categorical_accuracy: 0.9822 - val_loss: 0.2690 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 61/100\n",
      "57/57 - 22s - loss: 0.0567 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2707 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 62/100\n",
      "57/57 - 22s - loss: 0.0592 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.2703 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 63/100\n",
      "57/57 - 22s - loss: 0.0604 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.2699 - val_sparse_categorical_accuracy: 0.9185\n",
      "Epoch 64/100\n",
      "57/57 - 22s - loss: 0.0551 - sparse_categorical_accuracy: 0.9824 - val_loss: 0.2692 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 65/100\n",
      "57/57 - 22s - loss: 0.0567 - sparse_categorical_accuracy: 0.9810 - val_loss: 0.2687 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 66/100\n",
      "57/57 - 22s - loss: 0.0570 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2690 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 67/100\n",
      "57/57 - 22s - loss: 0.0574 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2683 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 68/100\n",
      "57/57 - 22s - loss: 0.0576 - sparse_categorical_accuracy: 0.9802 - val_loss: 0.2691 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 69/100\n",
      "57/57 - 23s - loss: 0.0563 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2688 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 70/100\n",
      "57/57 - 40s - loss: 0.0571 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.2692 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 71/100\n",
      "57/57 - 47s - loss: 0.0586 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.2691 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 72/100\n",
      "57/57 - 48s - loss: 0.0584 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.2690 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 73/100\n",
      "57/57 - 48s - loss: 0.0567 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2688 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 74/100\n",
      "57/57 - 47s - loss: 0.0561 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.2686 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 75/100\n",
      "57/57 - 48s - loss: 0.0559 - sparse_categorical_accuracy: 0.9817 - val_loss: 0.2686 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 76/100\n",
      "57/57 - 48s - loss: 0.0605 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.2683 - val_sparse_categorical_accuracy: 0.9188\n",
      "Epoch 77/100\n",
      "57/57 - 48s - loss: 0.0562 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 78/100\n",
      "57/57 - 44s - loss: 0.0561 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 79/100\n",
      "57/57 - 22s - loss: 0.0579 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 80/100\n",
      "57/57 - 22s - loss: 0.0560 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 81/100\n",
      "57/57 - 22s - loss: 0.0579 - sparse_categorical_accuracy: 0.9808 - val_loss: 0.2682 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 82/100\n",
      "57/57 - 22s - loss: 0.0565 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2682 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 83/100\n",
      "57/57 - 22s - loss: 0.0583 - sparse_categorical_accuracy: 0.9815 - val_loss: 0.2682 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 84/100\n",
      "57/57 - 22s - loss: 0.0564 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 85/100\n",
      "57/57 - 22s - loss: 0.0575 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 86/100\n",
      "57/57 - 22s - loss: 0.0577 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 87/100\n",
      "57/57 - 22s - loss: 0.0561 - sparse_categorical_accuracy: 0.9814 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 88/100\n",
      "57/57 - 22s - loss: 0.0552 - sparse_categorical_accuracy: 0.9821 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 89/100\n",
      "57/57 - 22s - loss: 0.0562 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 90/100\n",
      "57/57 - 22s - loss: 0.0589 - sparse_categorical_accuracy: 0.9809 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 91/100\n",
      "57/57 - 23s - loss: 0.0553 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 92/100\n",
      "57/57 - 22s - loss: 0.0577 - sparse_categorical_accuracy: 0.9812 - val_loss: 0.2680 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 93/100\n",
      "57/57 - 22s - loss: 0.0552 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n",
      "Epoch 94/100\n",
      "57/57 - 34s - loss: 0.0547 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.2681 - val_sparse_categorical_accuracy: 0.9189\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fit model 2519.995712768996\n",
      "29/29 [==============================] - 8s 80ms/step\n",
      "Prediction Accuracy: 0.9560546875\n",
      "[[1874    0    0    0]\n",
      " [   2 1519    2  317]\n",
      " [   0    0  920    0]\n",
      " [   0    1    3 2764]] \n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     1.000     0.999      1874\n",
      "           1      0.999     0.826     0.904      1840\n",
      "           2      0.995     1.000     0.997       920\n",
      "           3      0.897     0.999     0.945      2768\n",
      "\n",
      "    accuracy                          0.956      7402\n",
      "   macro avg      0.972     0.956     0.962      7402\n",
      "weighted avg      0.960     0.956     0.955      7402\n",
      " \n",
      "\n",
      "\n",
      "Total Time 2530.415923501001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m_count = 0\n",
    "print('Model Number %s' % m_count)\n",
    "append_list_time1 = pd.DataFrame()\n",
    "with mirrored_strategy.scope():\n",
    "    history, y_preds, res_list = train_func(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=Val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.0125\n",
    "Drop_rate = 0.3\n",
    "Repeats = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number 0\n",
      "Time till start of create model 0.00023112000053515658\n",
      "Receptive Field Size: 2541\n",
      "x.shape= (None, 16)\n",
      "model.x = (None, 4000, 1)\n",
      "model.y = (None, 4)\n",
      "Filter Length: 16\n",
      "Kernel Size: 11\n",
      "Dilation: 7\n",
      "Learning Rate: 0.0125\n",
      "Dropout Rate: 0.3\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 4000, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Model (TCN)                  (None, 16)                37936     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 68        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 38,004\n",
      "Trainable params: 37,556\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Time to create model 0.6457900200039148\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "45/45 - 35s - loss: 0.9979 - sparse_categorical_accuracy: 0.5328 - val_loss: 3.9722 - val_sparse_categorical_accuracy: 0.3286\n",
      "Epoch 2/100\n",
      "45/45 - 23s - loss: 0.6817 - sparse_categorical_accuracy: 0.6871 - val_loss: 5.1443 - val_sparse_categorical_accuracy: 0.3385\n",
      "Epoch 3/100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m_count = 0\n",
    "print('Model Number %s' % m_count)\n",
    "append_list_time1 = pd.DataFrame()\n",
    "with mirrored_strategy.scope():\n",
    "    history, y_preds, res_list = train_func(train_data=train_dataset,\n",
    "                                            test_dataset=test_dataset,\n",
    "                                            Val_dataset=Val_dataset,\n",
    "                                            predict_labels=predict_labels, \n",
    "                                            filt_num=16,\n",
    "                                            kernel_num=11,\n",
    "                                            dilation=7,\n",
    "                                            stack=1,\n",
    "                                            learn_r=Learning_Rate,\n",
    "                                            drop_rate=Drop_rate,\n",
    "                                            runs=Repeats,\n",
    "                                            Model_num = m_count, \n",
    "                                            segment_length = segment_length)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_list_time1.sort_values(by=['segment length', 'kernel size', 'stacks', 'dilation', 'filters'], ascending=[True, False, False, False, True])\n",
    "append_list_time1.to_csv(r'{}.csv'.format('List_Check_300'), index = False, header=['segment length', 'filters', 'kernel size', 'stacks', 'dropout', 'lr', 'dilation','Training Time', 'train loss', 'train acc', 'eval acc','Eval Acc','C1 correct', 'C1 as C2', 'C1 as C3', 'C1 as C4','C2 as C1', 'C2 correct', 'C2 as C3', 'C2 as C4','C3 as C1', 'C3 as C2', 'C3 correct', 'C3 as C4','C4 as C1', 'C4 as C2', 'C4 as C3', 'C4 correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment length</th>\n",
       "      <th>filters</th>\n",
       "      <th>kernel size</th>\n",
       "      <th>stacks</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>dilation</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>train loss</th>\n",
       "      <th>train acc</th>\n",
       "      <th>...</th>\n",
       "      <th>C2 as C3</th>\n",
       "      <th>C2 as C4</th>\n",
       "      <th>C3 as C1</th>\n",
       "      <th>C3 as C2</th>\n",
       "      <th>C3 correct</th>\n",
       "      <th>C3 as C4</th>\n",
       "      <th>C4 as C1</th>\n",
       "      <th>C4 as C2</th>\n",
       "      <th>C4 as C3</th>\n",
       "      <th>C4 correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>659.189826</td>\n",
       "      <td>0.055559</td>\n",
       "      <td>0.981654</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2400</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>1619.093654</td>\n",
       "      <td>0.013442</td>\n",
       "      <td>0.995525</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>3123.541723</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>0.993533</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2400</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>8</td>\n",
       "      <td>623.930027</td>\n",
       "      <td>0.049244</td>\n",
       "      <td>0.984067</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>191</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2400</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>8</td>\n",
       "      <td>638.345629</td>\n",
       "      <td>0.026409</td>\n",
       "      <td>0.990862</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2400</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>8</td>\n",
       "      <td>1099.084539</td>\n",
       "      <td>0.016896</td>\n",
       "      <td>0.994494</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2400</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>421.807514</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.992994</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2400</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>562.381346</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.996275</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2400</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>1392.870528</td>\n",
       "      <td>0.076700</td>\n",
       "      <td>0.971087</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3600</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>8</td>\n",
       "      <td>642.300958</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.996116</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3600</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>8</td>\n",
       "      <td>655.977071</td>\n",
       "      <td>0.012332</td>\n",
       "      <td>0.995704</td>\n",
       "      <td>...</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3600</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>8</td>\n",
       "      <td>1276.249551</td>\n",
       "      <td>0.010877</td>\n",
       "      <td>0.996732</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3600</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>6</td>\n",
       "      <td>578.695284</td>\n",
       "      <td>0.045843</td>\n",
       "      <td>0.984394</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3600</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>6</td>\n",
       "      <td>1865.557837</td>\n",
       "      <td>0.136559</td>\n",
       "      <td>0.941093</td>\n",
       "      <td>...</td>\n",
       "      <td>97</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3600</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>6</td>\n",
       "      <td>7356.620306</td>\n",
       "      <td>0.340099</td>\n",
       "      <td>0.858011</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3600</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>312.023654</td>\n",
       "      <td>0.030663</td>\n",
       "      <td>0.990791</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3600</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>598.915033</td>\n",
       "      <td>0.032090</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3600</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>1648.590427</td>\n",
       "      <td>0.094453</td>\n",
       "      <td>0.965154</td>\n",
       "      <td>...</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2400</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>1027.365963</td>\n",
       "      <td>0.017617</td>\n",
       "      <td>0.994599</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2400</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>1486.824010</td>\n",
       "      <td>0.007128</td>\n",
       "      <td>0.997923</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3600</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>714.731912</td>\n",
       "      <td>0.007371</td>\n",
       "      <td>0.997966</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3600</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>1278.473227</td>\n",
       "      <td>0.014882</td>\n",
       "      <td>0.995524</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>3</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3600</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>7</td>\n",
       "      <td>3492.792724</td>\n",
       "      <td>0.163025</td>\n",
       "      <td>0.936608</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    segment length  filters  kernel size  stacks  dropout      lr  dilation  \\\n",
       "0             2400       16            6       2    0.175  0.0125         7   \n",
       "1             2400       32            6       2    0.175  0.0125         7   \n",
       "2             2400       64            6       2    0.175  0.0125         7   \n",
       "3             2400       16            6       1    0.175  0.0125         8   \n",
       "4             2400       32            6       1    0.175  0.0125         8   \n",
       "5             2400       64            6       1    0.175  0.0125         8   \n",
       "6             2400       16           11       1    0.175  0.0125         7   \n",
       "7             2400       32           11       1    0.175  0.0125         7   \n",
       "8             2400       64           11       1    0.175  0.0125         7   \n",
       "9             3600       16            8       1    0.175  0.0125         8   \n",
       "10            3600       32            8       1    0.175  0.0125         8   \n",
       "11            3600       64            8       1    0.175  0.0125         8   \n",
       "12            3600       16           16       2    0.175  0.0125         6   \n",
       "13            3600       32           16       2    0.175  0.0125         6   \n",
       "14            3600       64           16       2    0.175  0.0125         6   \n",
       "15            3600       16           16       1    0.175  0.0125         7   \n",
       "16            3600       32           16       1    0.175  0.0125         7   \n",
       "17            3600       64           16       1    0.175  0.0125         7   \n",
       "18            2400       16            6       2    0.175  0.0125         7   \n",
       "19            2400       32            6       2    0.175  0.0125         7   \n",
       "20            3600       16           16       1    0.175  0.0125         7   \n",
       "21            3600       32           16       1    0.175  0.0125         7   \n",
       "22            3600       64           16       1    0.175  0.0125         7   \n",
       "\n",
       "    Training Time  train loss  train acc  ...  C2 as C3  C2 as C4  C3 as C1  \\\n",
       "0      659.189826    0.055559   0.981654  ...         1        80         0   \n",
       "1     1619.093654    0.013442   0.995525  ...         2         1         0   \n",
       "2     3123.541723    0.018711   0.993533  ...         5         0         0   \n",
       "3      623.930027    0.049244   0.984067  ...         0         3         0   \n",
       "4      638.345629    0.026409   0.990862  ...         1         7         0   \n",
       "5     1099.084539    0.016896   0.994494  ...         0         0         0   \n",
       "6      421.807514    0.022398   0.992994  ...         1         2         0   \n",
       "7      562.381346    0.011449   0.996275  ...         2         1         0   \n",
       "8     1392.870528    0.076700   0.971087  ...         8         8         0   \n",
       "9      642.300958    0.012497   0.996116  ...         7         0         0   \n",
       "10     655.977071    0.012332   0.995704  ...        56         0         0   \n",
       "11    1276.249551    0.010877   0.996732  ...        51         0         0   \n",
       "12     578.695284    0.045843   0.984394  ...        50        46         0   \n",
       "13    1865.557837    0.136559   0.941093  ...        97        25         0   \n",
       "14    7356.620306    0.340099   0.858011  ...        67        16         0   \n",
       "15     312.023654    0.030663   0.990791  ...         0         0         0   \n",
       "16     598.915033    0.032090   0.989969  ...        33        28         0   \n",
       "17    1648.590427    0.094453   0.965154  ...        45        57         0   \n",
       "18    1027.365963    0.017617   0.994599  ...         2        11         0   \n",
       "19    1486.824010    0.007128   0.997923  ...         2        98         0   \n",
       "20     714.731912    0.007371   0.997966  ...         0        24         0   \n",
       "21    1278.473227    0.014882   0.995524  ...         6         4         0   \n",
       "22    3492.792724    0.163025   0.936608  ...         7        67         0   \n",
       "\n",
       "    C3 as C2  C3 correct  C3 as C4  C4 as C1  C4 as C2  C4 as C3  C4 correct  \n",
       "0          0         200         0         0         0        29         270  \n",
       "1          0         200         0         0         0         0         299  \n",
       "2          0         200         0         0        18         0         281  \n",
       "3          0         191         9         0         0        17         282  \n",
       "4          0         200         0         0         0        22         277  \n",
       "5          0         200         0         0         1         5         293  \n",
       "6          0         200         0         0         0         1         298  \n",
       "7          1         199         0         0         0         4         295  \n",
       "8          0         200         0         0         3         0         296  \n",
       "9          0         132         0         0         0         0         199  \n",
       "10         0         132         0         0         0         6         193  \n",
       "11         0         100        32         0        49        15         135  \n",
       "12         0         132         0         0         0        18         181  \n",
       "13         0         132         0         0         0         0         199  \n",
       "14         0         132         0         1        43         7         148  \n",
       "15         0         132         0         0         0         1         198  \n",
       "16         0         132         0         0         0         0         199  \n",
       "17         0         132         0         0        19         0         180  \n",
       "18         0         200         0        13        64         7         215  \n",
       "19         0         200         0         0         2        50         247  \n",
       "20         0         132         0         0        68        33          98  \n",
       "21         0         131         1         0       121         3          75  \n",
       "22         0         132         0         0        25         1         173  \n",
       "\n",
       "[23 rows x 28 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_list_time1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
