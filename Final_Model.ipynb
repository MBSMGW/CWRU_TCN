{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.7/site-packages (3.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow-addons[tensorflow-gpu] in /usr/local/lib/python3.7/site-packages (0.12.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: multivariate_cwru in /usr/local/lib/python3.7/site-packages (0.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (4.60.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/site-packages (from scipy) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from sklearn) (0.23.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (3.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (7.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: typeguard in /usr/local/lib/python3.7/site-packages (2.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "%pip install keras-tcn --no-dependencies\n",
    "\n",
    "%pip install tensorflow-addons[tensorflow-gpu] --no-dependencies\n",
    "\n",
    "%pip install pandas --upgrade\n",
    "\n",
    "%pip install multivariate_cwru\n",
    "\n",
    "%pip install tqdm\n",
    "\n",
    "%pip install scipy\n",
    "\n",
    "%pip install sklearn\n",
    "\n",
    "%pip install matplotlib\n",
    "\n",
    "%pip install typeguard\n",
    "\n",
    "\n",
    "tf.config.list_physical_devices(device_type=None)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "print(visible_devices)\n",
    "num_GPU = len(tf.config.experimental.list_physical_devices('/physical_device:GPU:0'))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "#Deep Learning pkgs\n",
    "from tensorflow.keras import backend as K, Input, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras.metrics import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TensorBoard, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.activations import swish\n",
    "K.backend()\n",
    "\n",
    "# Python\n",
    "from IPython.core.debugger import set_trace\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "#Project Specific\n",
    "import tcn_ed\n",
    "from tcn_ed import TCN, tcn_full_summary, compiled_tcn\n",
    "from help_pre import create_data_batcht as Create_Batch, create_pred_batch\n",
    "import multivariate_cwru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import(dir_p, use_type, high_freq = False, b_size=1024):\n",
    "    segment_length = 2400\n",
    "    target = 'label'\n",
    "    cwd = os.getcwd()\n",
    "    col_ran = [*range(2400)]\n",
    "    d_t = dict()\n",
    "    d_t['label'] = 'int8'\n",
    "    d_t['Unnamed: 0'] = 'int8'\n",
    "    for col in col_ran:\n",
    "        d_t['%s' % col] =  'float16'\n",
    "    \n",
    "    if str(cwd) == str(dir_p):\n",
    "        dir_path=Path('./')\n",
    "    else: dir_path = dir_p\n",
    "    \n",
    "    if use_type == 'train':\n",
    "        if high_freq == False:\n",
    "            train_frame = pd.read_csv((r'{}.csv').format('train_df_no48'), dtype=(d_t))\n",
    "            test_frame = pd.read_csv((r'{}.csv').format('test_df_no48'), dtype=(d_t))\n",
    "            \n",
    "        if high_freq:\n",
    "            train_frame = pd.read_csv((r'{}.csv').format('train_df_w48'), dtype=(d_t))\n",
    "            test_frame = pd.read_csv((r'{}.csv').format('test_df_w48'), dtype=(d_t))\n",
    "            \n",
    "        train_frame = train_frame.set_index(train_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "        train_target = train_frame.pop('label')\n",
    "        train_x = train_frame.to_numpy().reshape(len(train_frame), segment_length, 1)\n",
    "        train_y = train_target.to_numpy().reshape(-1, 1)\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_optimization.apply_default_optimizations = True\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(b_size).with_options(options).cache()\n",
    "        test_frame = test_frame.set_index(test_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "        test_target = test_frame.pop('label')\n",
    "        test_x = test_frame.to_numpy().reshape(len(test_frame), segment_length, 1)\n",
    "        test_y = test_target.to_numpy().reshape(-1, 1)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(b_size).with_options(options).cache()\n",
    "        print('class balance of train frame: %s' % train_target.value_counts())\n",
    "        print('class balance of validation (test) frame: %s' % test_target.value_counts())\n",
    "        out_1 = train_dataset\n",
    "        out_2 = test_dataset\n",
    "            \n",
    "    if use_type == 'pred':\n",
    "        if high_freq == False:\n",
    "            val_frame = pd.read_csv((r'{}.csv').format('pred_df_no48'), dtype=d_t)\n",
    "            \n",
    "        if high_freq:\n",
    "            val_frame == pd.read_csv((r'{}.csv').format('pred_df_w48'), dtype=d_t)\n",
    "\n",
    "\n",
    "        val_frame = val_frame.set_index(val_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "        val_target = val_frame.pop('label')\n",
    "        print('class balance of train frame: %s' % val_target.value_counts())\n",
    "        val_x = val_frame.to_numpy().reshape(len(val_frame), segment_length, 1)\n",
    "        val_y = val_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "        options = tf.data.Options()\n",
    "        options.experimental_optimization.apply_default_optimizations = True\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(b_size).with_options(options).cache()\n",
    "\n",
    "        out_1 = dataset\n",
    "        out_2 = val_y\n",
    "        \n",
    "    return out_1, out_2\n",
    "\n",
    "def adjust_dilations(dilations: list):\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations\n",
    "        \n",
    "def pred_accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.cast(y_pred, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "def create_model(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "\n",
    "    input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "    def get_opt():\n",
    "        if opt == 'adam':\n",
    "            return optimizers.Adam(lr=lr)\n",
    "        elif opt == 'rmsprop':\n",
    "            return optimizers.RMSprop(lr=lr)\n",
    "        else:\n",
    "            raise Exception('Only Adam and RMSProp are available here')\n",
    "\n",
    "    x = TCN(nb_filters=filt_num,\n",
    "            kernel_size=kernel_num,\n",
    "            nb_stacks=stack,\n",
    "            dilations=[2 ** i for i in range(dilation)],\n",
    "            padding='causal',\n",
    "            use_skip_connections=use_skip,\n",
    "            dropout_rate=drop_rate,\n",
    "            return_sequences=False,\n",
    "            activation='swish', \n",
    "            kernel_initializer='he_uniform',\n",
    "            use_batch_norm=False,\n",
    "            use_layer_norm=False,\n",
    "            use_weight_norm=True,\n",
    "            name='Model')(input_layer)\n",
    "\n",
    "    tcn = TCN(nb_filters=filt_num,\n",
    "              kernel_size=kernel_num,\n",
    "              nb_stacks=stack,\n",
    "              dilations=[2 ** i for i in range(dilation)],\n",
    "              padding='causal',\n",
    "              use_skip_connections=use_skip,\n",
    "              dropout_rate=drop_rate,\n",
    "              return_sequences=False,\n",
    "              activation='swish', \n",
    "              kernel_initializer='he_uniform',\n",
    "              use_batch_norm=False,\n",
    "              use_layer_norm=False,\n",
    "              use_weight_norm=True,\n",
    "              name='Model')\n",
    "\n",
    "    print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "    # classification\n",
    "    x = Dense(num_classes)(x)\n",
    "    x = Activation('softmax', dtype='float32')(x)\n",
    "    output_layer = x\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    # https://github.com/keras-team/keras/pull/11373\n",
    "    # It's now in Keras@master but still not available with pip.\n",
    "    # TODO remove later.\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    print('model.x = {}'.format(input_layer.shape))\n",
    "    print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "    print('Filter Length: %s' % filt_num)\n",
    "    print('Kernel Size: %s' % kernel_num)\n",
    "    print('Dilation: %s' % dilation)\n",
    "    print('Learning Rate: %s' % lr)\n",
    "    print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_func(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num,\n",
    "               callback):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    history = model.fit(train_data,\n",
    "              epochs=runs,\n",
    "              verbose=1,\n",
    "              callbacks=callback,\n",
    "              validation_data=test_dataset)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    y_preds = model.predict(Val_dataset,\n",
    "                                   verbose=1,\n",
    "                                   callbacks=callback,\n",
    "                                  )\n",
    "    y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "    predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "    #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "    #y_argmax = []\n",
    "    #i=0\n",
    "    perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "    Accuracy_test = float(perc_score)\n",
    "    print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "    con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "    class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "    print(con_mat, '\\n\\n')\n",
    "    print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "    \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return model, history, y_preds, append_list2_in\n",
    "\n",
    "def create_model2(num_feat,\n",
    "                 num_classes,\n",
    "                 filt_num,\n",
    "                 kernel_num,\n",
    "                 stack, dilation,\n",
    "                 lr,\n",
    "                 drop_rate,\n",
    "                 segment_length,\n",
    "                 use_skip,\n",
    "                 opt):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    #x =Sequential()\n",
    "\n",
    "\n",
    "    input_layer = Input(shape=(segment_length, num_feat))\n",
    "\n",
    "    def get_opt():\n",
    "        if opt == 'adam':\n",
    "            return optimizers.Adam(lr=lr)\n",
    "        elif opt == 'rmsprop':\n",
    "            return optimizers.RMSprop(lr=lr)\n",
    "        else:\n",
    "            raise Exception('Only Adam and RMSProp are available here')\n",
    "\n",
    "    x = TCN(nb_filters=filt_num,\n",
    "            kernel_size=kernel_num,\n",
    "            nb_stacks=stack,\n",
    "            dilations=[2 ** i for i in range(dilation)],\n",
    "            padding='causal',\n",
    "            use_skip_connections=use_skip,\n",
    "            dropout_rate=drop_rate,\n",
    "            return_sequences=False,\n",
    "            activation='swish', \n",
    "            kernel_initializer='he_uniform',\n",
    "            use_batch_norm=True,\n",
    "            use_layer_norm=False,\n",
    "            use_weight_norm=False,\n",
    "            name='Model')(input_layer)\n",
    "\n",
    "    tcn = TCN(nb_filters=filt_num,\n",
    "              kernel_size=kernel_num,\n",
    "              nb_stacks=stack,\n",
    "              dilations=[2 ** i for i in range(dilation)],\n",
    "              padding='causal',\n",
    "              use_skip_connections=use_skip,\n",
    "              dropout_rate=drop_rate,\n",
    "              return_sequences=False,\n",
    "              activation='swish', \n",
    "              kernel_initializer='he_uniform',\n",
    "              use_batch_norm=True,\n",
    "              use_layer_norm=False,\n",
    "              use_weight_norm=False,\n",
    "              name='Model')\n",
    "\n",
    "    print('Receptive Field Size: %s' % tcn.receptive_field)\n",
    "\n",
    "\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "\n",
    "    # classification\n",
    "    x = Dense(num_classes)(x)\n",
    "    x = Activation('softmax', dtype='float32')(x)\n",
    "    output_layer = x\n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    # https://github.com/keras-team/keras/pull/11373\n",
    "    # It's now in Keras@master but still not available with pip.\n",
    "    # TODO remove later.\n",
    "\n",
    "    #with mirrored_strategy.scope():\n",
    "    model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    print('model.x = {}'.format(input_layer.shape))\n",
    "    print('model.y = {}'.format(output_layer.shape))\n",
    "\n",
    "    print('Filter Length: %s' % filt_num)\n",
    "    print('Kernel Size: %s' % kernel_num)\n",
    "    print('Dilation: %s' % dilation)\n",
    "    print('Learning Rate: %s' % lr)\n",
    "    print('Dropout Rate: %s' % drop_rate)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_func2(train_data,\n",
    "               test_dataset,\n",
    "               Val_dataset,\n",
    "               predict_labels,\n",
    "               segment_length,\n",
    "               runs,\n",
    "               filt_num,\n",
    "               kernel_num,\n",
    "               dilation,\n",
    "               stack,\n",
    "               learn_r,\n",
    "               drop_rate,\n",
    "               Model_num,\n",
    "               callback):\n",
    "    \n",
    "    \n",
    "        \n",
    "    time1 = timer()\n",
    "        \n",
    "    working_dir = Path('.')\n",
    "    model_path = working_dir / 'Model' / ('Model_{}_k{}_s{}_di{}_dr{}_L{}.H5'.format(Model_num, kernel_num, stack, dilation, drop_rate, segment_length))\n",
    "    \n",
    "    \n",
    "\n",
    "    #seg_length = None\n",
    "    time_out = timer()-time1\n",
    "    print('Time till start of create model %s' % time_out)\n",
    "    time2 = timer()\n",
    "    model = create_model2(num_feat=1,\n",
    "                         num_classes=4,\n",
    "                         filt_num=filt_num,\n",
    "                         kernel_num=kernel_num,\n",
    "                         stack=stack,\n",
    "                         dilation=dilation,\n",
    "                         lr=learn_r,\n",
    "                         drop_rate=drop_rate,\n",
    "                         segment_length = segment_length,\n",
    "                         use_skip=True,\n",
    "                         opt='adam')\n",
    "\n",
    "    time3 = timer()\n",
    "    time_out = time3-time2\n",
    "    print('Time to create model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    history = model.fit(train_data,\n",
    "              epochs=runs,\n",
    "              verbose=1,\n",
    "              callbacks=callback,\n",
    "              validation_data=test_dataset,\n",
    "              use_multiprocessing=True)\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.show()\n",
    "    time_out = timer()-time3\n",
    "    print('Time to fit model %s' % time_out)\n",
    "    #with mirrored_strategy.scope():\n",
    "    y_preds = model.predict(Val_dataset,\n",
    "                                   verbose=1,\n",
    "                                   callbacks=callback,\n",
    "                                  )\n",
    "    y_argmax2 = np.argmax(y_preds,axis=1)\n",
    "    predict_labels = np.squeeze(predict_labels[:], axis=1)\n",
    "    #for pred1, pred2, pred3, pred4 in y_pred:\n",
    "    #y_argmax = []\n",
    "    #i=0\n",
    "    perc_score = tf.dtypes.cast((sum(pred_accuracy(predict_labels, y_argmax2))/(len(predict_labels))), tf.float16)\n",
    "    Accuracy_test = float(perc_score)\n",
    "    print('Prediction Accuracy: %s' % Accuracy_test)\n",
    "    con_mat = metrics.confusion_matrix(predict_labels, y_argmax2)\n",
    "    class_report = metrics.classification_report(predict_labels, y_argmax2, digits=3)\n",
    "    print(con_mat, '\\n\\n')\n",
    "    print(class_report, '\\n\\n')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    d = {'segment length':[segment_length], 'filters': [filt_num], 'kernel size': [kernel_num], 'stacks':[stack], 'dropout': [drop_rate], 'lr': [learn_r], 'dilation': [dilation], 'Training Time': time_out, 'train loss': [history.history['loss'][-1]], 'train acc': [history.history['sparse_categorical_accuracy'][-1]], 'eval acc': [history.history['val_sparse_categorical_accuracy'][-1]], 'Eval Acc': [Accuracy_test],'C1 correct': [con_mat[0][0]], 'C1 as C2':[con_mat[0][1]], 'C1 as C3':[con_mat[0][2]], 'C1 as C4':[con_mat[0][3]],'C2 as C1': [con_mat[1][0]], 'C2 correct':[con_mat[1][1]], 'C2 as C3':[con_mat[1][2]], 'C2 as C4':[con_mat[1][3]],'C3 as C1': [con_mat[2][0]], 'C3 as C2':[con_mat[2][1]], 'C3 correct':[con_mat[2][2]], 'C3 as C4':[con_mat[2][3]],'C4 as C1': [con_mat[3][0]], 'C4 as C2':[con_mat[3][1]], 'C4 as C3':[con_mat[3][2]], 'C4 correct':[con_mat[3][3]]}\n",
    "    \n",
    "    \n",
    "    append_list2_in = pd.DataFrame.from_dict(d)\n",
    "       \n",
    "    time_out = timer()-time1\n",
    "    print('Total Time %s' % time_out)\n",
    "    \n",
    "    return model, history, y_preds, append_list2_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ran = [*range(2400)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class balance of train frame: 3    57451\n",
      "1    28917\n",
      "2    28747\n",
      "0     9656\n",
      "Name: label, dtype: int64\n",
      "class balance of validation (test) frame: 3    30229\n",
      "1    12081\n",
      "2    11013\n",
      "0     9650\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = csv_import(dir_p = os.getcwd(), use_type='train', high_freq = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class balance of train frame: 3    4770\n",
      "1    2384\n",
      "2    2384\n",
      "0    1612\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_dataset, predict_labels = csv_import(dir_p = os.getcwd(), use_type = 'pred' , high_freq = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.5,   \n",
    "                              patience=3, \n",
    "                              min_lr=0.000000015,\n",
    "                              verbose=1, \n",
    "                              cooldown=5)\n",
    "\n",
    "ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                         min_delta=0.00001, \n",
    "                                         patience=25, \n",
    "                                         verbose=0, \n",
    "                                         mode='auto', \n",
    "                                         baseline=None, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "callback = [reduce_lr, ES_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 1024 #normally 2048 but 512 on colab\n",
    "step_size = 300\n",
    "step_length=step_size\n",
    "segment_length = 2400\n",
    "split_perc = 0.3\n",
    "working_dir = Path('.')\n",
    "DATA_PATH = Path(\"./Datatest/CWRU\")\n",
    "save_model_path = working_dir / 'Model' \n",
    "\n",
    "for path in [DATA_PATH, save_model_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_list_time1=pd.DataFrame()\n",
    "Learning_Rate = 0.02\n",
    "Drop_rate = 0\n",
    "Repeats = 75\n",
    "m_count=1\n",
    "print('Model Number %s' % m_count)\n",
    "model1, history1, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=4,\n",
    "                                        kernel_num=8,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class balance of train frame: 3    57451\n",
      "1    28917\n",
      "2    28747\n",
      "0     9656\n",
      "Name: label, dtype: int64\n",
      "class balance of validation (test) frame: 3    30229\n",
      "1    12081\n",
      "2    11013\n",
      "0     9650\n",
      "Name: label, dtype: int64\n",
      "class balance of train frame: 3    4770\n",
      "2    2384\n",
      "1    2384\n",
      "0    1612\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "segment_length = 2400\n",
    "target = 'label'\n",
    "cwd = os.getcwd()\n",
    "col_ran = [*range(2400)]\n",
    "b_size = 512\n",
    "d_t = dict()\n",
    "d_t['label'] = 'int8'\n",
    "d_t['Unnamed: 0'] = 'int8'\n",
    "for col in col_ran:\n",
    "    d_t['%s' % col] =  'float32'\n",
    "\n",
    "\n",
    "\n",
    "train_frame = pd.read_csv((r'{}.csv').format('train_df_w48'), dtype=(d_t))\n",
    "test_frame = pd.read_csv((r'{}.csv').format('test_df_w48'), dtype=(d_t))\n",
    "\n",
    "train_frame = train_frame.set_index(train_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "train_target = train_frame.pop('label')\n",
    "train_x = train_frame.to_numpy().reshape(len(train_frame), segment_length, 1)\n",
    "train_y = train_target.to_numpy().reshape(-1, 1)\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(b_size).with_options(options).cache()\n",
    "test_frame = test_frame.set_index(test_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "test_target = test_frame.pop('label')\n",
    "test_x = test_frame.to_numpy().reshape(len(test_frame), segment_length, 1)\n",
    "test_y = test_target.to_numpy().reshape(-1, 1)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(b_size).with_options(options).cache()\n",
    "print('class balance of train frame: %s' % train_target.value_counts())\n",
    "print('class balance of validation (test) frame: %s' % test_target.value_counts())\n",
    "out_1 = train_dataset\n",
    "out_2 = test_dataset\n",
    "\n",
    "val_frame = pd.read_csv((r'{}.csv').format('pred_df_no48'), dtype=d_t)\n",
    "\n",
    "\n",
    "val_frame = val_frame.set_index(val_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val_target = val_frame.pop('label')\n",
    "print('class balance of train frame: %s' % val_target.value_counts())\n",
    "val_x = val_frame.to_numpy().reshape(len(val_frame), segment_length, 1)\n",
    "val_y = val_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(b_size).with_options(options).cache()\n",
    "\n",
    "val_dataset = dataset\n",
    "out_2 = val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.3,   \n",
    "                              patience=10, \n",
    "                              min_lr=0.0000000015,\n",
    "                              verbose=0, \n",
    "                              cooldown=5)\n",
    "\n",
    "ES_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                         min_delta=0.00001, \n",
    "                                         patience=25, \n",
    "                                         verbose=0, \n",
    "                                         mode='auto', \n",
    "                                         baseline=None, \n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "callback = [reduce_lr, ES_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number 1\n",
      "Time till start of create model 4.165699647273868e-05\n",
      "[2021-05-12 05:44:21.076 tensorflow-2-3-gpu-ml-g4dn-4xlarge-f1d995902f6ec07d1f4d0a5fa443:4722 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-05-12 05:44:21.100 tensorflow-2-3-gpu-ml-g4dn-4xlarge-f1d995902f6ec07d1f4d0a5fa443:4722 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Receptive Field Size: 2541\n",
      "x.shape= (None, 8)\n",
      "model.x = (None, 2400, 1)\n",
      "model.y = (None, 4)\n",
      "Filter Length: 8\n",
      "Kernel Size: 11\n",
      "Dilation: 7\n",
      "Learning Rate: 0.02\n",
      "Dropout Rate: 0\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2400, 1)]         0         \n",
      "_________________________________________________________________\n",
      "Model (TCN)                  (None, 8)                 9816      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 9,852\n",
      "Trainable params: 9,628\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Time to create model 1.544660188999842\n",
      "Epoch 1/75\n",
      "122/122 [==============================] - 107s 875ms/step - loss: 0.8089 - sparse_categorical_accuracy: 0.6225 - val_loss: 86.7096 - val_sparse_categorical_accuracy: 0.4765\n",
      "Epoch 2/75\n",
      "122/122 [==============================] - 104s 855ms/step - loss: 0.3418 - sparse_categorical_accuracy: 0.8506 - val_loss: 12.6647 - val_sparse_categorical_accuracy: 0.4807\n",
      "Epoch 3/75\n",
      "122/122 [==============================] - 103s 847ms/step - loss: 0.1367 - sparse_categorical_accuracy: 0.9469 - val_loss: 12.0627 - val_sparse_categorical_accuracy: 0.5004\n",
      "Epoch 4/75\n",
      "122/122 [==============================] - 104s 853ms/step - loss: 0.0515 - sparse_categorical_accuracy: 0.9822 - val_loss: 5.9207 - val_sparse_categorical_accuracy: 0.5219\n",
      "Epoch 5/75\n",
      "122/122 [==============================] - 104s 856ms/step - loss: 0.0334 - sparse_categorical_accuracy: 0.9883 - val_loss: 6.2144 - val_sparse_categorical_accuracy: 0.5321\n",
      "Epoch 6/75\n",
      "122/122 [==============================] - 103s 848ms/step - loss: 0.0255 - sparse_categorical_accuracy: 0.9915 - val_loss: 6.5016 - val_sparse_categorical_accuracy: 0.5522\n",
      "Epoch 7/75\n",
      "122/122 [==============================] - 104s 855ms/step - loss: 0.0176 - sparse_categorical_accuracy: 0.9939 - val_loss: 5.3560 - val_sparse_categorical_accuracy: 0.6125\n",
      "Epoch 8/75\n",
      "122/122 [==============================] - 104s 854ms/step - loss: 0.0248 - sparse_categorical_accuracy: 0.9915 - val_loss: 3.1106 - val_sparse_categorical_accuracy: 0.6468\n",
      "Epoch 9/75\n",
      "122/122 [==============================] - ETA: 0s - loss: 0.0085 - sparse_categorical_accuracy: 0.9972"
     ]
    }
   ],
   "source": [
    "append_list_time1=pd.DataFrame()\n",
    "Learning_Rate = 0.02\n",
    "Drop_rate = 0\n",
    "Repeats = 75\n",
    "m_count=0\n",
    "tf.keras.backend.clear_session()\n",
    "Drop_rate=0\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model2, history2, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=8,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "Drop_rate=0.1\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model2, history2, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=8,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment_length = 8192\n",
    "target = 'label'\n",
    "cwd = os.getcwd()\n",
    "col_ran = [*range(8192)]\n",
    "d_t = dict()\n",
    "d_t['label'] = 'int8'\n",
    "d_t['Unnamed: 0'] = 'int8'\n",
    "for col in col_ran:\n",
    "    d_t['%s' % col] =  'float16'\n",
    "\n",
    "\n",
    "\n",
    "train_frame = pd.read_csv((r'{}.csv').format('train_df_w48_8192'), dtype=(d_t))\n",
    "test_frame = pd.read_csv((r'{}.csv').format('test_df_w48_8192'), dtype=(d_t))\n",
    "\n",
    "train_frame = train_frame.set_index(train_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "train_target = train_frame.pop('label')\n",
    "train_x = train_frame.to_numpy().reshape(len(train_frame), segment_length, 1)\n",
    "train_y = train_target.to_numpy().reshape(-1, 1)\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(b_size).with_options(options).cache()\n",
    "test_frame = test_frame.set_index(test_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "test_target = test_frame.pop('label')\n",
    "test_x = test_frame.to_numpy().reshape(len(test_frame), segment_length, 1)\n",
    "test_y = test_target.to_numpy().reshape(-1, 1)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(b_size).with_options(options).cache()\n",
    "print('class balance of train frame: %s' % train_target.value_counts())\n",
    "print('class balance of validation (test) frame: %s' % test_target.value_counts())\n",
    "out_1 = train_dataset\n",
    "out_2 = test_dataset\n",
    "\n",
    "val_frame = pd.read_csv((r'{}.csv').format('pred_df_no48_8192'), dtype=d_t)\n",
    "\n",
    "\n",
    "val_frame = val_frame.set_index(val_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val_target = val_frame.pop('label')\n",
    "print('class balance of train frame: %s' % val_target.value_counts())\n",
    "val_x = val_frame.to_numpy().reshape(len(val_frame), segment_length, 1)\n",
    "val_y = val_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(b_size).with_options(options).cache()\n",
    "\n",
    "val_dataset = dataset\n",
    "predict_labels = val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "Drop_rate=0\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model1, history1, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=8,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "Drop_rate=0.1\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model1, history1, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=8,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = 8192\n",
    "target = 'label'\n",
    "cwd = os.getcwd()\n",
    "col_ran = [*range(8192)]\n",
    "d_t = dict()\n",
    "d_t['label'] = 'float32'\n",
    "d_t['Unnamed: 0'] = 'float32'\n",
    "for col in col_ran:\n",
    "    d_t['%s' % col] =  'float32'\n",
    "\n",
    "\n",
    "\n",
    "train_frame = pd.read_csv((r'{}.csv').format('train_df_w48_8192'), dtype=(d_t))\n",
    "test_frame = pd.read_csv((r'{}.csv').format('test_df_w48_8192'), dtype=(d_t))\n",
    "\n",
    "train_frame = train_frame.set_index(train_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "train_target = train_frame.pop('label')\n",
    "train_x = train_frame.to_numpy().reshape(len(train_frame), segment_length, 1)\n",
    "train_y = train_target.to_numpy().reshape(-1, 1)\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)).batch(b_size).with_options(options).cache()\n",
    "test_frame = test_frame.set_index(test_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "test_target = test_frame.pop('label')\n",
    "test_x = test_frame.to_numpy().reshape(len(test_frame), segment_length, 1)\n",
    "test_y = test_target.to_numpy().reshape(-1, 1)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(b_size).with_options(options).cache()\n",
    "print('class balance of train frame: %s' % train_target.value_counts())\n",
    "print('class balance of validation (test) frame: %s' % test_target.value_counts())\n",
    "out_1 = train_dataset\n",
    "out_2 = test_dataset\n",
    "\n",
    "val_frame = pd.read_csv((r'{}.csv').format('pred_df_no48_8192'), dtype=d_t)\n",
    "\n",
    "\n",
    "val_frame = val_frame.set_index(val_frame.iloc[:, 0]).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val_target = val_frame.pop('label')\n",
    "print('class balance of train frame: %s' % val_target.value_counts())\n",
    "val_x = val_frame.to_numpy().reshape(len(val_frame), segment_length, 1)\n",
    "val_y = val_target.to_numpy().reshape(-1, 1)\n",
    "\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)).batch(b_size).with_options(options).cache()\n",
    "\n",
    "val_dataset = dataset\n",
    "out_2 = val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "Drop_rate=0\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model1, history1, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=8,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "Drop_rate=0.1\n",
    "m_count += 1\n",
    "print('Model Number %s' % m_count)\n",
    "model1, history1, y_preds, res_list = train_func2(train_data=train_dataset,\n",
    "                                        test_dataset=test_dataset,\n",
    "                                        Val_dataset=val_dataset,\n",
    "                                        predict_labels=predict_labels, \n",
    "                                        filt_num=8,\n",
    "                                        kernel_num=11,\n",
    "                                        dilation=7,\n",
    "                                        stack=1,\n",
    "                                        learn_r=Learning_Rate,\n",
    "                                        drop_rate=Drop_rate,\n",
    "                                        runs=Repeats,\n",
    "                                        Model_num = m_count, \n",
    "                                        segment_length = segment_length,\n",
    "                                        callback = callback)\n",
    "\n",
    "append_list_time1 = append_list_time1.append(res_list, ignore_index=True)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-southeast-2:452832661640:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
